[TOC]



# Hadoop生态

## yarn

由RM，nodemanager，application master，container组成。yarn目前为止仅支持cpu和内存两种资源管理和调度

### 组件职责：

#### ResourceManager (RM)

ResourceManager 是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件组成：

- Scheduler：资源调度器，主要功能和特点如下：
  - 负责将资源分配给各种正在运行的应用程序，这些应用程序受到容量、队列等限制；
  - Scheduler 是纯调度程序，不会监视或跟踪应用程序的状态；
  - 由于应用程序故障或硬件故障，它不提供有关重新启动失败任务的保证；
  - Scheduler 根据应用程序的资源需求来执行其调度功能，它是基于资源容器的抽象概念来实现的，容器（Container）内包含内存、CPU、磁盘、网络等因素；
  - Scheduler 是一个可插拔的插件（即可配置），负责在各种队列、应用程序等之间对集群资源进行区分。当前支持的Scheduler类包括：FairScheduler、FifoScheduler、CapacityScheduler；
- Application Manager：负责接受 job 提交请求，为应用程序分配第一个 Container 以运行 ApplicationMaster，并提供失败时重新启动运行着 ApplicationMaster 的 Container 的服务。

#### ApplicationMaster（AM）

当用户提交一个应用程序时，将启动一个被称为 ApplcationMaster 的轻量级进程的实例，用以协调应用程序内所有任务的执行。它的主要工作包括：

- 向 ResourceManager 申请并以容器（Container）的形式提供计算资源；
- 管理在容器内运行的任务：
  - 跟踪任务的状态并监视它们的执行；
  - 遇到失败时，重新启动失败的任务；
  - 推测性的运行缓慢的任务以及计算应用计数器的总值。

#### NodeManager（NM）

NodeManager 进程运行在集群中的节点上，是每个节点上的资源和任务管理器。它的主要功能包括：

- 接收 ResourceManager 的资源分配请求，并为应用程序分配具体的 Container；
- 定时地向 ResourceManager 汇报本节点上的资源使用情况和各个 Container 的运行状态，以确保整个集群平稳运行；
- 管理每个 Container 的生命周期；
- 管理每个节点上的日志；
- 接收并处理来自 ApplicationMaster 的 Container 启动/停止等请求。

#### Container（容器）

Container 是 Yarn 中的资源抽象，是执行具体应用的基本单位，它包含了某个 NodeManager 节点上的多维度资源，如内存、CPU、磁盘和网络 IO，当然目前仅支持内存和 CPU。任何一个 Job 或应用程序必须运行在一个或多个 Container 中，在 Yarn 中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用，ApplicationMaster 需要自己去找 NodeManager 请求分配具体的 Container。

Container 和集群节点的关系是：一个节点会运行多个 Container，但一个 Container 不会跨节点。

### Yarn 任务提交流程

![img](https://tva1.sinaimg.cn/large/008eGmZEly1goclh97ohij30gu0b2myc.jpg)

Yarn 任务执行流程主要包括如下几个步骤：

1. 客户端向 RM 发出请求；
2. RM 返回一个 ApplicationID 作为回应；
3. 客户端向 RM 回应 Application Submission Context（ASC）和 Container Launch Context（CLC）信息。其中 ASC 包括 ApplicationID、user、queue，以及其它一些启动 AM 相关的信息，CLC 包含了资源请求数（内存与CPU），Job 文件，安全 token，以及其它一些用于在 NM 上启动 AM的信息；
4. 当 ResourceManager 接受到 ASC 后，它会调度一个合适的 container 来启动 AM，这个 container 经常被称做 container 0。AM 需要请求其它的 container 来运行任务，如果没有合适的 RM，AM 就不能启动。当有合适的 container 时，RM 发请求到合适的 NM 上，来启动 AM。这时候，AM 的 PRC 与监控的 URL 就已经建立了；
5. 当 AM 启动起来后，RM 回应给 AM 集群的最小与最大资源等信息。这时 AM 必须决定如何使用那么当前可用的资源。Yarn不像那些请求固定资源的 scheduler，它能够根据集群的当前状态动态调整；
6. AM 根据从 RM 那里得知的可使用的资源，它会请求一些一定数目的 container。这个请求可以是非常具体的包括具有多个资源最小值的 Container（例如额外的内存等）；
7. RM 将会根据调度策略，尽可能的满足 AM 申请的 container；
8. AM 根据分配的信息，去找NM启动对应的 container。

### Yarn的内存管理

yarn允许用户配置每个节点上**可用的物理内存资源**（yarn web页面上显示的资源量），一个节点上内存会被若干个服务共享，比如一部分给了yarn，一部分给了hdfs，一部分给了hbase等，yarn配置的只是自己可用的，

#### 内存配置参数如下：

**yarn.nodemanager.resource.memory-mb**
**表示该节点上yarn可以使用的物理内存总量，默认是8192m，注意，如果你的节点内存资源不够8g，则需要调减这个值，yarn不会智能的探测节点物理内存总量。**
yarn.nodemanager.vmem-pmem-ratio
任务使用1m物理内存最多可以使用虚拟内存量，默认是2：1
yarn.nodemanager.pmem-check-enabled
是否启用一个线程检查每个任务证使用的物理内存量，如果任务超出了分配值，则直接将其kill，默认是true。
yarn.nodemanager.vmem-check-enabled
是否启用一个线程检查每个任务证使用的虚拟内存量，如果任务超出了分配值，则直接将其kill，默认是true。
yarn.scheduler.minimum-allocation-mb
单个任务可以使用最小物理内存量，默认1024m，如果一个任务申请物理内存量少于该值，则该对应值改为这个数。
yarn.scheduler.maximum-allocation-mb
单个任务可以申请的最多的内存量，默认8192m

### Yarn cpu管理

目前cpu被划分为虚拟cpu，这里的虚拟cpu是yarn自己引入的概念，初衷是考虑到不同节点cpu性能可能不同，每个cpu具有计算能力也是不一样的，比如，某个物理cpu计算能力可能是另外一个物理cpu的2倍，这时候，你可以通过为第一个物理cpu多配置几个虚拟cpu弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟cpu个数。在yarn中，

#### cpu相关配置参数如下：

**yarn.nodemanager.resource.cpu-vcores**
**表示该节点上yarn可使用的虚拟cpu个数，默认是8个，注意，目前推荐将该值为与物理cpu核数相同。如果你的节点cpu合数不够8个，则需要调减小这个值，而yarn不会智能的探测节点物理cpu总数。**
yarn.scheduler.minimum-allocation-vcores
单个任务可申请最小cpu个数，默认1，如果一个任务申请的cpu个数少于该数，则该对应值被修改为这个数
yarn.scheduler.maximum-allocation-vcores
单个任务可以申请最多虚拟cpu个数，默认是32.

## HDFS

### 写操作

![image.png](https://tva1.sinaimg.cn/large/008eGmZEly1gocn984s6aj30tg0jlk8m.jpg)

有一个文件FileA，200M大小。Client将FileA写入到HDFS上。

HDFS按默认配置。

HDFS分布在三个机架上Rack1，Rack2，Rack3。

**a.** Client将FileA按128M分块。分成两块，block1和Block2;

**b.** Client向nameNode发送写数据请求，如图蓝色虚线①------>。

**c.** NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。

​    Block1: host2,host1,host3

​    Block2: host7,host8,host4

​    原理：

​        NameNode具有RackAware机架感知功能，这个可以配置。

​        若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。

​        若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。

**d.** client向DataNode发送block1；发送过程是以流式写入。

​    流式写入过程，

​        **1>**将128M的block1按64k的package划分;

​        **2>**然后将第一个package发送给host2;

​        **3>**host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；

​        **4>**host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。

​        **5>**以此类推，如图红线实线所示，直到将block1发送完毕。

​        **6>**host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。

​        **7>**client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线

​        **8>**发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。

​        **9>**发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。

​        **10>**client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。

**分析：**通过写过程，我们可以了解到：

​    **①**写1T文件，我们需要3T的存储，3T的网络流量带宽。

​    **②**在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。

​    **③**挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。

写入原理--具体流程

​       1.客户端通过调用DistributedFileSystem的create方法，创建一个新的文件。

　　2.DistributedFileSystem通过RPC（远程过程调用）调用NameNode，去创建一个没有blocks关联的新文件。创建前，NameNode 会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NameNode就会记录下新文件，否则就会抛出IO异常。

　　3.前两步结束后会返回 FSDataOutputStream 的对象，和读文件的时候相似，FSDataOutputStream 被封装成 DFSOutputStream，DFSOutputStream 可以协调 NameNode和 DataNode。客户端开始写数据到DFSOutputStream,DFSOutputStream会把数据切成一个个小packet，然后排成队列 data queue。

　　4.DataStreamer 会去处理接受 data queue，它先问询 NameNode 这个新的 block 最适合存储的在哪几个DataNode里，比如重复数是3，那么就找到3个最适合的 DataNode，把它们排成一个 pipeline。DataStreamer 把 packet 按队列输出到管道的第一个 DataNode 中，第一个 DataNode又把 packet 输出到第二个 DataNode 中，以此类推。

　　5.DFSOutputStream 还有一个队列叫 ack queue，也是由 packet 组成，等待DataNode的收到响应，当pipeline中的所有DataNode都表示已经收到的时候，这时akc queue才会把对应的packet包移除掉。

　　6.客户端完成写数据后，调用close方法关闭写入流。

　　7.DataStreamer 把剩余的包都刷到 pipeline 里，然后等待 ack 信息，收到最后一个 ack 后，通知 DataNode 把文件标示为已完成。

### 读操作

![image.png](https://tva1.sinaimg.cn/large/008eGmZEly1gocn9ez5ahj30t40jnwp4.jpg)

读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。 

那么，读操作流程为：

**a.** client向namenode发送读请求。

**b.** namenode查看Metadata信息，返回fileA的block的位置。

​    block1:host2,host1,host3

​    block2:host7,host8,host4

**c.** block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；

上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：

**优选读取本机架上的数据**。

读取原理--具体步骤：

1、首先调用FileSystem对象的open方法，其实获取的是一个DistributedFileSystem的实例。

2、DistributedFileSystem通过RPC(远程过程调用)获得文件的第一批block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面。

3、前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream就会找出离客户端最近的datanode并连接datanode。

4、数据从datanode源源不断的流向客户端。

5、如果第一个block块的数据读完了，就会关闭指向第一个block块的datanode连接，接着读取下一个block块。这些操作对客户端来说是透明的，从客户端的角度来看只是读一个持续不断的流。

6、如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的block块都读完，这时就会关闭掉所有的流。

## Hive

[Hive常见开窗函数(工作常用，面试常问)](https://mp.weixin.qq.com/s/0sHA_fiBTWjALpkjWA3Waw)

# 计算引擎

## MR计算模型

**MapReduce的执行步骤：**

1、Map任务处理

　　1.1 读取HDFS中的文件。每一行解析成一个<k,v>。每一个键值对调用一次map函数。<0,hello you>   <10,hello me>

　　1.2 覆盖map()，接收1.1产生的<k,v>，进行处理，转换为新的<k,v>输出。　　　　　　　　　　**<hello,1> <you,1> <hello,1> <me,1>**

　　1.3 对1.2输出的<k,v>进行分区。分区机制：对map中的key做hash，对reduce个数取模

　　1.4 对不同分区中的数据进行排序（按照k）、分组。分组指的是相同key的value放到一个集合中。　排序后：**<hello,1> <hello,1> <me,1> <you,1>**  分组后：**<hello,{1,1}><me,{1}><you,{1}>**

　　1.5 （可选）对分组后的数据进行归约。详见《[Combiner](http://www.cnblogs.com/ahu-lichang/p/6657572.html)》

2、Reduce任务处理

　　2.1 多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（**shuffle**）详见《[shuffle过程分析](http://www.cnblogs.com/ahu-lichang/p/6665242.html)》

　　2.2 对多个map的输出进行合并、排序。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑，　**<hello,2> <me,1> <you,1>**

　　　　处理后，产生新的<k,v>输出。

　　2.3 对reduce输出的<k,v>写到HDFS中。

## MR shuffle阶段

其实就是之前《[MapReduce的原理及执行过程](http://www.cnblogs.com/ahu-lichang/p/6645074.html)》中的步骤2.1。**多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。**

![img](https://tva1.sinaimg.cn/large/008eGmZEgy1godhjpg6m1j30p00dq1bc.jpg)

 

![img](https://tva1.sinaimg.cn/large/008eGmZEgy1godhjqzhwcj30ph0bs7c6.jpg)

 

 Map端：

　　1、在map端首先接触的是InputSplit，在InputSplit中含有DataNode中的数据，每一个InputSplit都会分配一个Mapper任务，Mapper任务结束后产生<K2,V2>的输出，**这些输出先存放在缓存中**，每个map有一个环形内存缓冲区，用于存储任务的输出。默认大小100MB（io.sort.mb属性），一旦达到阀值0.8(io.sort.spil l.percent)，一个后台线程就把内容写到(spill)Linux本地磁盘中的指定目录（mapred.local.dir）下的新建的一个溢出写文件。（注意：map过程的输出是写入本地磁盘而不是HDFS，但是一开始数据并不是直接写入磁盘而是缓冲在内存中，缓存的好处就是**减少磁盘I/O的开销，提高合并和排序的速度**。又因为默认的内存缓冲大小是100M（当然这个是可以配置的），所以**在编写map函数的时候要尽量减少内存的使用，为shuffle过程预留更多的内存**，因为该过程是最耗时的过程。）

　　2、写磁盘前，要进行partition、sort和combine等操作。**MR默认分区机制：对map中的key做hash，对reduce个数取模，得到当前的目的reducer，**之后对不同分区的数据进行排序，如果有Combiner，还要对排序后的数据进行combine。等最后记录写完，将全部溢出文件合并为一个分区且排序的文件。（注意：在写磁盘的时候采用**压缩**的方式将map的输出结果进行压缩是一个减少网络开销很有效的方法！）

　　3、最后将磁盘中的数据送到Reduce中，从图中可以看出Map输出有三个分区，有一个分区数据被送到图示的Reduce任务中，剩下的两个分区被送到其他Reducer任务中。而图示的Reducer任务的其他的三个输入则来自其他节点的Map输出。



 Reduce端：

　　1、Copy阶段：Reducer通过Http方式得到输出文件的分区。

　　reduce端可能从n个map的结果中获取数据，而这些map的执行速度不尽相同，当其中一个map运行结束时，reduce就会从JobTracker中获取该信息。map运行结束后TaskTracker会得到消息，进而将消息汇报给　　JobTracker，reduce定时从JobTracker获取该信息，reduce端默认有5个数据复制线程从map端复制数据。

　　2、Merge阶段：如果形成多个磁盘文件会进行合并

　　从map端复制来的数据首先写到reduce端的缓存中，同样缓存占用到达一定阈值后会将数据写到磁盘中，同样会进行partition、combine、排序等过程。如果形成了多个磁盘文件还会进行合并，最后一次合并的结果作为reduce的输入而不是写入到磁盘中。

　　3、Reducer的参数：最后将合并后的结果作为输入传入Reduce任务中。（注意：当Reducer的输入文件确定后，整个Shuffle操作才最终结束。之后就是Reducer的执行了，最后Reducer会把结果存到HDFS上。）

# SparkSQL

## 小文件产生原因和控制

### 小文件概念

储于HDFS中小文件，即指文件的大小远小于HDFS上块（dfs.block.size）大小的文件。

### 小文件问题的影响

- 大量的小文件会给Hadoop集群的扩展性和性能带来严重的影响。NameNode在内存中维护整个文件系统的元数据镜像，用户HDFS的管理；其中每个HDFS文件元信息（位置，大小，分块等）对象约占150字节，如果小文件过多，会占用大量内存，直接影响NameNode的性能。相对的，HDFS读写小文件也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。如果NameNode在宕机中恢复，也需要更多的时间从元数据文件中加载。
- 另一方面，也会给Spark SQL等查询引擎造成查询性能的损耗，大量的数据分片信息以及对应产生的Task元信息也会给Spark Driver的内存造成压力，带来单点问题。此外，入库操作最后的commit job操作，在Spark Driver端单点做，很容易出现单点的性能问题。

### 查找小文件hive表

```shell
hadoop fs -count /dtInsight/hive/warehouse/aecc_mall.db/* |awk '$2>800 && $4!~/select/ {print $2,$3/1024/1024,$4}'
```

### Spark小文件产生的过程

#### 1.数据源本身就含大量小文件。

#### 2.动态分区插入数据。没有Shuffle的情况下

输入端有多少个逻辑分片，对应的HadoopRDD就会产生多少个HadoopPartition，**读取每个文件作为1个Partition，对应于Spark作业的Task（个数为M），表分区字段数为N。最好的情况就是（M=N）&（所有task中的数据也是根据N来预先打散的），那就刚好写N个文件**；最差的情况下，每个Task中都有各个分区的记录，那文件数最终文件数将达到M * N个。这种情况下是极易产生小文件的。

比如我们拿TPCDS测试集中的store_sales进行举例， sql如下所示：

```sql
use tpcds_1t_parquet;
INSERT overwrite table store_sales partition ( ss_sold_date_sk ) 
SELECT * FROM   tpcds_1t_ext.et_store_sales;
```

首先我们得到其执行计划，如下所示，

```
== Physical Plan ==
InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -> None), true, false
+- HiveTableScan [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L], MetastoreRelation tpcds_1t_ext, et_store_sales
```

**store_sales的原生文件包含1616逻辑分片，对应生成1616 个Spark Task(M值)，插入动态分区表之后生成1824个数据分区加一个NULL值的分区(N值=1824+1)**，每个分区下都有可能生成1616个文件，这种情况下，最终的文件数量极有可能达到2949200（M * N）。1T的测试集store_sales也就大概300g，这种情况每个文件可能就零点几M。

#### 3.动态分区插入数据，有Shuffle的情况下

Spark作业的Task个数的M值就变成了spark.sql.shuffle.partitions（默认值200）这个参数值，文件数的算法和范围和无shuflle中基本一致。

**为了防止Shuffle阶段的数据倾斜我们可以在上面的sql中加上 distribute by rand()，这样我们的执行计划就变成下面。这种情况下，每个分区的文件数文件数受spark.sql.shuffle.partitions控制，结果表总文件数妥妥的就是spark.sql.shuffle.partitions * N(分区字段数)，因为rand函数一般会把数据打散的非常均匀。**

**当spark.sql.shuffle.partitions设置过大时，小文件问题就产生了；当spark.sql.shuffle.partitions设置过小时，任务的并行度就下降了，性能随之受到影响。**

最理想的情况当然是根据分区字段进行shuffle，在上面的sql中加上distribute by ss_sold_date_sk。把同一分区的记录都哈希到同一个分区中去，由一个Spark的Task进行写入，这样的话只会产生N个文件，在我们的case中store_sales，在1825个分区下各生成了一个数据文件。

但是这种情况下也容易出现数据倾斜的问题，比如双11的销售数据就很容易在这种情况下发生倾斜。

```
InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -> None), true, false
+- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L]
   +- Exchange(coordinator id: 1080882047) hashpartitioning(_nondeterministic#49, 2048), coordinator[target post-shuffle partition size: 67108864]
      +- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L, rand(4184439864130379921) AS _nondeterministic#49]
         +- HiveTableScan [ss_sold_date_sk#3L, ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25], MetastoreRelation tpcds_1t_ext, et_store_sales
```

### 解决Spark SQL产生小文件

- 可以尝试是否可以将两者结合使用， 在之前的sql上加上distribute by ss_sold_date_sk，cast(rand() * 5 as int)， 这个类似于我们处理数据倾斜问题时候给字段加上后缀的形式。

```sql
use tpcds_1t_parquet;
INSERT overwrite table store_sales partition        ( ss_sold_date_sk ) 
SELECT * FROM   tpcds_1t_ext.et_store_sales
distribute by ss_sold_date_sk, cast(rand() * 5 as int);
```

按照之前的推算，每个分区下将产生5个文件，同时null值倾斜部分的数据也被打散成五份进行计算，缓解了数据倾斜的问题 ，我们最终将得到1825 *5=9105个文件，如下所示

```
 1825     9105    247111074494 /user/kyuubi/hive_db/tpcds_1t_parquet.db/store_sales
```

如果我们将5改得更小，文件数也会越少，但相应的倾斜key的计算时间也会上去。

- 在我们知道那个分区键倾斜的情况下，我们也可以将入库的SQL拆成几个部分，比如我们store_sales是因为null值倾斜，我们就可以通过where ss_sold_date_sk is not null 和 where ss_sold_date_sk is null 将原始数据分成两个部分。前者可以基于分区字段进行分区，如distribute by ss_sold_date_sk；后者可以基于随机值进行分区，distribute by cast(rand() * 5 as int)，这样可以静态的将null值部分分成五个文件。

```sql
FROM   tpcds_1t_ext.et_store_sales 
where ss_sold_date_sk is not null
distribute by ss_sold_date_sk;
```

```sql
FROM   tpcds_1t_ext.et_store_sales 
where ss_sold_date_sk is null
distribute by distribute by cast(rand() * 5 as int);
```

- 对于倾斜部分的数据，我们可以开启Spark SQL的自适应功能，spark.sql.adaptive.enabled=true来动态调整每个相当于Spark的reduce端task处理的数据量，这样我们就不需要人为的感知随机值的规模了，我们可以直接

```sql
FROM   tpcds_1t_ext.et_store_sales 
where ss_sold_date_sk is null
distribute by distribute by rand() ;
```

然后Spark在Shuffle 阶段会自动的帮我们将数据尽量的合并成spark.sql.adaptive.shuffle.targetPostShuffleInputSize（默认64m）的大小，以减少输出端写文件线程的总量，最后减少个数。

### 解决方案总结

对于spark.sql.adaptive.shuffle.targetPostShuffleInputSize参数而言，我们也可以设置成为dfs.block.size的大小，这样可以做到和块对齐，文件大小可以设置的最为合理。

1. 对于原始数据进行按照分区字段进行shuffle，可以规避小文件问题。但有可能引入数据倾斜的问题；

2. 可以通过distribute by ss_sold_date_sk, cast(rand() * N as int)，N值可以在文件数量和倾斜度之间做权衡；

3. 知道倾斜键的情况下，可以将原始数据分成几个部分处理，不倾斜的按照分区键shuffle，倾斜部分可以按照rand函数来shuffle；

4. 对于Spark 2.4 以上版本的用户，也可以使用HINT 详情，链接如下：

   https://issues.apache.org/jira/browse/SPARK-24940

5. 对于Spark 3.0 以上版本的用户，可以使用自适应查询（AQE）功能，设置spark.sql.adaptive.enabled和spark.sql.adaptive.coalescePartitions.enabled为true，Spark就会在计算过程中自动帮助用户合并小文件，更加方便和智能；

### 小文件测试案例（待重新整理）

> rand函数语法: rand() ,rand(int seed)
>
> 返回值: double
>
> 说明:返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列

> 小文件测试参考：https://blog.csdn.net/wangzhenbeyond/article/details/84315366 
>
> **根据 distribute by random_fileNum 中 的 random_fileNum 与 spark.sql.shuffle.partitions 做hash,相同hash值会聚合到同一个reduce中，从而控制最终文件数**，如果rand(1) ,理论上无论partitions多少，两者hash值都唯一，结果应该只有一个文件产出，结论就是rand(1) 与 spark.sql.shuffle.partitions 做hash,相同hash值会聚合到同一个reduce中，从而控制最终文件数

执行SQL，创建复制表 CREATE TABLE ld_test_cp_part LIKE  dwd_log_sslvpn_ipaddress_di;

#### shuffle.partitions固定，rand(N)函数不固定N值，文件数受spark.sql.shuffle.partitions控制

```sql
# 指定spark.sql.shuffle.partitions=30
INSERT OVERWRITE TABLE ld_test_cp_part PARTITION (pt)
SELECT *
FROM  dwd_log_sslvpn_ipaddress_di
distribute by rand(); 
```

分区插入，由于rand(N)函数不固定N值，测试结果和rand函数无关，rand只用来打乱数据分布，使数据尽可能分布均匀，文件数受spark.sql.shuffle.partitions控制

- 

https://www.jianshu.com/p/ddd2382a738a  INSERT overwrite table xxx partition ( 分区字段 ) distribute by 分区字段, cast(rand() * N as int) 

每个分区下将产生N个文件，null值倾斜部分的数据也会被打散成N份进行计算，缓解了数据倾斜的问题，如果我们将5改得更小，文件数也会越少，但相应的倾斜key的计算时间也会上去。那如果不加分区字段来分发，是什么结果

## 慢任务



## 常用参数调优（看一部分）

https://blog.csdn.net/rlnLo2pNEfx9c/article/details/79103670
https://blog.csdn.net/rlnLo2pNEfx9c/article/details/89117446?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159419627919195264516441%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=159419627919195264516441&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_blog_v1-6-89117446.pc_v2_rank_blog_v1&utm_term=spark
https://blog.csdn.net/maizi1045/article/details/90319855
https://blog.csdn.net/yalongwan01/article/details/101511580?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase

https://blog.csdn.net/u013411339/article/details/99375921

```sql
//1.下列Hive参数对Spark同样起作用。
set hive.exec.dynamic.partition=true; // 是否允许动态生成分区
set hive.exec.dynamic.partition.mode=nonstrict; // 是否容忍指定分区全部动态生成
set hive.exec.max.dynamic.partitions = 100; // 动态生成的最多分区数

//2.运行行为
set spark.sql.autoBroadcastJoinThreshold; // 大表 JOIN 小表，小表做广播的阈值
set spark.dynamicAllocation.enabled; // 开启动态资源分配
set spark.dynamicAllocation.maxExecutors; //开启动态资源分配后，最多可分配的Executor数
set spark.dynamicAllocation.minExecutors; //开启动态资源分配后，最少可分配的Executor数
set spark.sql.shuffle.partitions; // 需要shuffle是mapper端写出的partition个数
set spark.sql.adaptive.enabled; // 是否开启调整partition功能，如果开启，spark.sql.shuffle.partitions设置的partition可能会被合并到一个reducer里运行
set spark.sql.adaptive.shuffle.targetPostShuffleInputSize; //开启spark.sql.adaptive.enabled后，两个partition的和低于该阈值会合并到一个reducer
set spark.sql.adaptive.minNumPostShufflePartitions; // 开启spark.sql.adaptive.enabled后，最小的分区数
set spark.hadoop.mapreduce.input.fileinputformat.split.maxsize; //当几个stripe的大小大于该值时，会合并到一个task中处理

//3.executor能力
set spark.executor.memory; // executor用于缓存数据、代码执行的堆内存以及JVM运行时需要的内存
set spark.yarn.executor.memoryOverhead; //Spark运行还需要一些堆外内存，直接向系统申请，如数据传输时的netty等。
set spark.sql.windowExec.buffer.spill.threshold; //当用户的SQL中包含窗口函数时，并不会把一个窗口中的所有数据全部读进内存，而是维护一个缓存池，当池中的数据条数大于该参数表示的阈值时，spark将数据写到磁盘
set spark.executor.cores; //单个executor上可以同时运行的task数
```

## Spark任务执行过程

![图片](https://tva1.sinaimg.cn/large/008eGmZEgy1gofs6uuzxlj30u00nb12q.jpg)

详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。

在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。

Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。

当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。

因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。

task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。

## SparkSQL Catalyst解析过程（主要）

[spark sql解析过程中对tree的遍历（源码详解）](https://mp.weixin.qq.com/s/mQWKyzzMBMoypKySdEaBjw)

https://blog.csdn.net/lisenyeahyeah/category_8281549.html

https://blog.csdn.net/vipshop_fin_dev/article/details/83472641

## Spark特性：推测执行、RBO及CBO（主要）

[Spark SQL 性能优化再进一步 CBO 基于代价的优化](http://www.jasongj.com/spark/cbo/)

## SparkSQL的几种Join类型（主要）

**join的hive表小于默认阈值10M,也没有触发 mapjoin[解决方案]**
spark在join的时候,用来判断一个表的大小是否达到了10M这个限制,是不会去计算这个表在hdfs上的具体的文件大小的,而是使用hive metadata中的信息，当有些hive没有totalSize这个信息的时候,spark就会用sortMergeJoin来做join了,可以使用下面的命令重新生成metadata信息:

ANALYZE TABLE dm_sdk_mapping.app_category_mapping COMPUTE STATISTICS

### join选择策略：

**org.apache.spark.sql.execution.SparkStrategies.JoinSelection中的**

```scala
    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {

      // --- BroadcastHashJoin --------------------------------------------------------------------

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)
        if canBuildRight(joinType) && canBroadcast(right) =>
        Seq(joins.BroadcastHashJoinExec(
          leftKeys, rightKeys, joinType, BuildRight, condition, planLater(left), planLater(right)))

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)
        if canBuildLeft(joinType) && canBroadcast(left) =>
        Seq(joins.BroadcastHashJoinExec(
          leftKeys, rightKeys, joinType, BuildLeft, condition, planLater(left), planLater(right)))

      // --- ShuffledHashJoin ---------------------------------------------------------------------

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)
         if !conf.preferSortMergeJoin && canBuildRight(joinType) && canBuildLocalHashMap(right)
           && muchSmaller(right, left) ||
           !RowOrdering.isOrderable(leftKeys) =>
        Seq(joins.ShuffledHashJoinExec(
          leftKeys, rightKeys, joinType, BuildRight, condition, planLater(left), planLater(right)))

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)
         if !conf.preferSortMergeJoin && canBuildLeft(joinType) && canBuildLocalHashMap(left)
           && muchSmaller(left, right) ||
           !RowOrdering.isOrderable(leftKeys) =>
        Seq(joins.ShuffledHashJoinExec(
          leftKeys, rightKeys, joinType, BuildLeft, condition, planLater(left), planLater(right)))

      // --- SortMergeJoin ------------------------------------------------------------

      case ExtractEquiJoinKeys(joinType, leftKeys, rightKeys, condition, left, right)
        if RowOrdering.isOrderable(leftKeys) =>
        joins.SortMergeJoinExec(
          leftKeys, rightKeys, joinType, condition, planLater(left), planLater(right)) :: Nil
```

其中conf.preferSortMergeJoin

org.apache.spark.sql.internal.SQLConf

```scala
  val PREFER_SORTMERGEJOIN = SQLConfigBuilder("spark.sql.join.preferSortMergeJoin")
    .internal()
    .doc("When true, prefer sort merge join over shuffle hash join.")
    .booleanConf
    .createWithDefault(true)
```

配置spark.sql.join.preferSortMergeJoin，默认为true，即是否优先使用SortMergeJoin；

**可以看到join实现主要有3种，即BroadcastHashJoinExec、ShuffledHashJoinExec和SortMergeJoinExec，优先级为**

- **1 如果canBroadcast，则BroadcastHashJoinExec；**

- **2 如果spark.sql.join.preferSortMergeJoin=false，则ShuffledHashJoinExec；**

- **3 否则为SortMergeJoinExec；**

  **其中BroadcastHashJoinExec和ShuffledHashJoinExec都会用到HashJoin，先看HashJoin：**

#### HashJoin

org.apache.spark.sql.execution.joins.HashJoin

```scala
  protected def join(
      streamedIter: Iterator[InternalRow],
      hashed: HashedRelation,
      numOutputRows: SQLMetric): Iterator[InternalRow] = {

    val joinedIter = joinType match {
      case _: InnerLike =>
        innerJoin(streamedIter, hashed)
      case LeftOuter | RightOuter =>
        outerJoin(streamedIter, hashed)
      case LeftSemi =>
        semiJoin(streamedIter, hashed)
      case LeftAnti =>
        antiJoin(streamedIter, hashed)
      case j: ExistenceJoin =>
        existenceJoin(streamedIter, hashed)
      case x =>
        throw new IllegalArgumentException(
          s"BroadcastHashJoin should not take $x as the JoinType")
    }

    val resultProj = createResultProjection
    joinedIter.map { r =>
      numOutputRows += 1
      resultProj(r)
    }
  }

  private def innerJoin(
      streamIter: Iterator[InternalRow],
      hashedRelation: HashedRelation): Iterator[InternalRow] = {
    val joinRow = new JoinedRow
    val joinKeys = streamSideKeyGenerator()
    streamIter.flatMap { srow =>
      joinRow.withLeft(srow)
      val matches = hashedRelation.get(joinKeys(srow))
      if (matches != null) {
        matches.map(joinRow.withRight(_)).filter(boundCondition)
      } else {
        Seq.empty
      }
    }
  }
```

这里只贴出内关联，即innerJoin，代码比较简单，注意这里是内存操作，会在单个partition内部进行；

##### BroadcastHashJoinExec

org.apache.spark.sql.execution.joins.BroadcastHashJoinExec

```scala
  protected override def doExecute(): RDD[InternalRow] = {
    val numOutputRows = longMetric("numOutputRows")

    val broadcastRelation = buildPlan.executeBroadcast[HashedRelation]()
    streamedPlan.execute().mapPartitions { streamedIter =>
      val hashed = broadcastRelation.value.asReadOnlyCopy()
      TaskContext.get().taskMetrics().incPeakExecutionMemory(hashed.estimatedSize)
      join(streamedIter, hashed, numOutputRows)
    }
  }
```

这里会将buildPlan广播出去，然后在streamedPlan上通过mapPartitions在1个分区内部进行join，join方法见HashJoin；

##### ShuffledHashJoinExec

org.apache.spark.sql.execution.joins.ShuffledHashJoinExec

```scala
  protected override def doExecute(): RDD[InternalRow] = {
    val numOutputRows = longMetric("numOutputRows")
    streamedPlan.execute().zipPartitions(buildPlan.execute()) { (streamIter, buildIter) =>
      val hashed = buildHashedRelation(buildIter)
      join(streamIter, hashed, numOutputRows)
    }
  }
```

join过程为先将两个rdd（streamedPlan和buildPlan）进行zipPartitions，然后在1个partition内部join，join方法见HashJoin；

#### SortMergeJoinExec

org.apache.spark.sql.execution.joins.SortMergeJoinExec

```scala
  protected override def doExecute(): RDD[InternalRow] = {
    val numOutputRows = longMetric("numOutputRows")

    left.execute().zipPartitions(right.execute()) { (leftIter, rightIter) =>
      val boundCondition: (InternalRow) => Boolean = {
        condition.map { cond =>
          newPredicate(cond, left.output ++ right.output).eval _
        }.getOrElse {
          (r: InternalRow) => true
        }
      }

      // An ordering that can be used to compare keys from both sides.
      val keyOrdering = newNaturalAscendingOrdering(leftKeys.map(_.dataType))
      val resultProj: InternalRow => InternalRow = UnsafeProjection.create(output, output)

      joinType match {
        case _: InnerLike =>
          new RowIterator {
            private[this] var currentLeftRow: InternalRow = _
            private[this] var currentRightMatches: ArrayBuffer[InternalRow] = _
            private[this] var currentMatchIdx: Int = -1
            private[this] val smjScanner = new SortMergeJoinScanner(
              createLeftKeyGenerator(),
              createRightKeyGenerator(),
              keyOrdering,
              RowIterator.fromScala(leftIter),
              RowIterator.fromScala(rightIter)
            )
            private[this] val joinRow = new JoinedRow

            if (smjScanner.findNextInnerJoinRows()) {
              currentRightMatches = smjScanner.getBufferedMatches
              currentLeftRow = smjScanner.getStreamedRow
              currentMatchIdx = 0
            }

            override def advanceNext(): Boolean = {
              while (currentMatchIdx >= 0) {
                if (currentMatchIdx == currentRightMatches.length) {
                  if (smjScanner.findNextInnerJoinRows()) {
                    currentRightMatches = smjScanner.getBufferedMatches
                    currentLeftRow = smjScanner.getStreamedRow
                    currentMatchIdx = 0
                  } else {
                    currentRightMatches = null
                    currentLeftRow = null
                    currentMatchIdx = -1
                    return false
                  }
                }
                joinRow(currentLeftRow, currentRightMatches(currentMatchIdx))
                currentMatchIdx += 1
                if (boundCondition(joinRow)) {
                  numOutputRows += 1
                  return true
                }
              }
              false
            }

            override def getRow: InternalRow = resultProj(joinRow)
          }.toScala
...
```

和ShuffledHashJoinExec一样，同样先zipPartitions，然后在1个partition内部根据joinType返回不同的RowIterator实现类，上边代码包含内关联实现，大部分工作通过SortMergeJoinScanner实现

org.apache.spark.sql.execution.joins.SortMergeJoinScanner

```scala
  final def findNextInnerJoinRows(): Boolean = {
    while (advancedStreamed() && streamedRowKey.anyNull) {
      // Advance the streamed side of the join until we find the next row whose join key contains
      // no nulls or we hit the end of the streamed iterator.
    }
    if (streamedRow == null) {
      // We have consumed the entire streamed iterator, so there can be no more matches.
      matchJoinKey = null
      bufferedMatches.clear()
      false
    } else if (matchJoinKey != null && keyOrdering.compare(streamedRowKey, matchJoinKey) == 0) {
      // The new streamed row has the same join key as the previous row, so return the same matches.
      true
    } else if (bufferedRow == null) {
      // The streamed row's join key does not match the current batch of buffered rows and there are
      // no more rows to read from the buffered iterator, so there can be no more matches.
      matchJoinKey = null
      bufferedMatches.clear()
      false
    } else {
      // Advance both the streamed and buffered iterators to find the next pair of matching rows.
      var comp = keyOrdering.compare(streamedRowKey, bufferedRowKey)
      do {
        if (streamedRowKey.anyNull) {
          advancedStreamed()
        } else {
          assert(!bufferedRowKey.anyNull)
          comp = keyOrdering.compare(streamedRowKey, bufferedRowKey)
          if (comp > 0) advancedBufferedToRowWithNullFreeJoinKey()
          else if (comp < 0) advancedStreamed()
        }
      } while (streamedRow != null && bufferedRow != null && comp != 0)
      if (streamedRow == null || bufferedRow == null) {
        // We have either hit the end of one of the iterators, so there can be no more matches.
        matchJoinKey = null
        bufferedMatches.clear()
        false
      } else {
        // The streamed row's join key matches the current buffered row's join, so walk through the
        // buffered iterator to buffer the rest of the matching rows.
        assert(comp == 0)
        bufferMatchingRows()
        true
      }
    }
  }

  /**
   * Advance the streamed iterator and compute the new row's join key.
   * @return true if the streamed iterator returned a row and false otherwise.
   */
  private def advancedStreamed(): Boolean = {
    if (streamedIter.advanceNext()) {
      streamedRow = streamedIter.getRow
      streamedRowKey = streamedKeyGenerator(streamedRow)
      true
    } else {
      streamedRow = null
      streamedRowKey = null
      false
    }
  }

  /**
   * Advance the buffered iterator until we find a row with join key that does not contain nulls.
   * @return true if the buffered iterator returned a row and false otherwise.
   */
  private def advancedBufferedToRowWithNullFreeJoinKey(): Boolean = {
    var foundRow: Boolean = false
    while (!foundRow && bufferedIter.advanceNext()) {
      bufferedRow = bufferedIter.getRow
      bufferedRowKey = bufferedKeyGenerator(bufferedRow)
      foundRow = !bufferedRowKey.anyNull
    }
    if (!foundRow) {
      bufferedRow = null
      bufferedRowKey = null
      false
    } else {
      true
    }
  }

  /**
   * Called when the streamed and buffered join keys match in order to buffer the matching rows.
   */
  private def bufferMatchingRows(): Unit = {
    assert(streamedRowKey != null)
    assert(!streamedRowKey.anyNull)
    assert(bufferedRowKey != null)
    assert(!bufferedRowKey.anyNull)
    assert(keyOrdering.compare(streamedRowKey, bufferedRowKey) == 0)
    // This join key may have been produced by a mutable projection, so we need to make a copy:
    matchJoinKey = streamedRowKey.copy()
    bufferedMatches.clear()
    do {
      bufferedMatches += bufferedRow.copy() // need to copy mutable rows before buffering them
      advancedBufferedToRowWithNullFreeJoinKey()
    } while (bufferedRow != null && keyOrdering.compare(streamedRowKey, bufferedRowKey) == 0)
  }
```

可以看到过程和二路归并排序Binary Merge Sort差不多；

附：RowIterator是一个抽象类，本质是一个接口，是一个常见的Iterator定义，如下：

org.apache.spark.sql.execution.RowIterator

```scala
abstract class RowIterator {
  /**
   * Advance this iterator by a single row. Returns `false` if this iterator has no more rows
   * and `true` otherwise. If this returns `true`, then the new row can be retrieved by calling
   * [[getRow]].
   */
  def advanceNext(): Boolean

  /**
   * Retrieve the row from this iterator. This method is idempotent. It is illegal to call this
   * method after [[advanceNext()]] has returned `false`.
   */
  def getRow: InternalRow

  /**
   * Convert this RowIterator into a [[scala.collection.Iterator]].
   */
  def toScala: Iterator[InternalRow] = new RowIteratorToScala(this)
}
```

参考：

[SparkSQL中的三种Join](https://blog.csdn.net/wlk_328909605/article/details/82933552)

[Spark中Join实现原理](https://www.cnblogs.com/barneywill/p/10187751.html)

[Spark SQL是如何选择join策略的？](https://mp.weixin.qq.com/s/kW3fU0CSeWMOAcI7boN-Tw)

[五种 Join 策略](https://mp.weixin.qq.com/s/HusOqNA-45lpf5GduLz-pA)

[Spark SQL 中 Broadcast Join 一定比 Shuffle Join 快？](https://mp.weixin.qq.com/s/5OBHLjRjOykuuaCqEthD4g)

## Spark内存管理模型（主要）

Spark 执行应用程序时, 会启动 Driver 和 Executor 两种 JVM 进程

### Driver 程序主要负责：

- 创建 Spark上下文；
- 提交 Spark作业（Job）并将 Job 转化为计算任务（Task）交给 Executor 计算；
- 协调各个 Executor 进程间任务调度。

### Executor 程序主要负责：

- 在工作节点上执行具体的计算任务（Task），并将计算结果返回给 Driver；
- 为需要持久化的 RDD 提供存储功能。

由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。

### Executor 内存管理

Executor 进程作为一个 JVM 进程，其内存管理建立在 JVM 的内存管理之上，**整个大致包含两种方式：堆内内存和堆外内存。**

![img](https://tva1.sinaimg.cn/large/008eGmZEly1gog0h8yz9wj30le0fidhc.jpg)

**一个 Executor 当中的所有 Task 是共享堆内内存的。一个 Work 中的多个 Executor 中的多个 Task 是共享堆外内存的。**

注意：默认的 Spark 是开启堆内内存的，配置参数：--executor-memory,但是默认的堆外内存是不开启的：

```
spark.memory.offHeap.enabled=true  # 开启堆外内存
spark.memory.offHeap.size =1024    # 分配堆外内存的大小
```

#### 堆内内存

- Spark 内存管理分为 静态内存管理 和 统一内存管理, Spark1.6 之前使用的是静态内存管理, Spark1.6 之后引入了统一内存管理。
- 静态内存管理中存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的, 但用户可以在应用程序启动前进行配置。
- 统一内存管理与静态内存管理的区别在于存储内存和执行内存共享同一块空间, 可以互相借用对方的空间。
- Spark1.6 及 1.6 之后的版本默认使用的是统一内存管理。
- 要想使用静态内存可以通过将参数 spark.memory.useLeagacyMode 设置为 true(默认为 false) 使用静态内存管理。

**堆内内存大小，是指 JVM 堆的内存大小，由 Spark 程序启动时的 -executor-memory 或 spark.executor.memory 参数配置。那 Spark 是如何管理堆内内存呢？Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看看申请内存和释放内存的具体流程：**

**申请内存：**

1. **Spark 在代码中 new 一个实例对象。**
2. **JVM 从堆内内存分配空间，创建对象并返回对象饮用。**
3. **Spark 保存该对象的引用，记录该对象占用的内存。**

**释放内存：**

1. **Spark 记录该对象释放的内存，删除该对象的引用。**
2. **等待 JVM 的垃圾回收机制释放该对象占用的堆内内存。**

##### 静态内存管理分布图

![image-20191026135001799](https://tva1.sinaimg.cn/large/008eGmZEgy1gog08b9vx8j30hx0g6jsg.jpg)

##### 统一内存管理分布图(1.6之后版本默认)



![img](https://tva1.sinaimg.cn/large/008eGmZEly1gog0n49hxfj31hl0u0n7h.jpg)JVM堆内内存大致可以分为以下四个模块：

- **Storage 内存：**主要用于存储 Spark 的 cache 数据，例如 RDD 的 cache，Broadcast 变量，Unroll 数据等。需要主要的是，unrolled 的数据如果内存不够，会存储在 driver 端。
- **Execution 内存：**用于存储 Spark task 执行过程中需要的对象，如 Shuffle、Join、Sort、Aggregation等计算过程中的临时数据。
- **User 内存：**分配 Spark Memory 剩余的内存，用户可以根据需要使用，可以存储 RDD transformations 需要的数据结构。
- **Reserved 内存：**这部分内存是预留给系统用的，固定不变。

#### 堆外内存

在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同如下图所示(以统一内存管理机制为例)，所有运行中的并发任务共享存储内存和执行内存。

![img](https://tva1.sinaimg.cn/large/008eGmZEly1gog1shj0hgj30r40d2myr.jpg)

### 统一内存管理动态占用机制

Spark 1.6 之后引入的统一内存管理机制，存储内存（Storeage Memory）和执行内存（Execution Memory）可以动态占用对方的空闲区域（如下图），在设定基本的的存储内存和执行内存区域（由 spark.storage.memoryFraction 参数控制）后，便确定了双方各自拥有的空间容量。统一内存管理的核心在内存区域的动态占用机制，其占用规则如下：

- 双方空间都不足时，则存储到硬盘；如己方空间不足而对方空余时，可借用对方的空间；（存储空间不足是指不足以放下一个完整的 Block）。
- 执行内存的空间被对方占用后，可让对方将占用的部分存储转存到硬盘，然后“归还”借用的空间。
- 存储内存的空间被对方占用后，无法让对方“归还”，因为需要考虑到 Shuffle 过程中很多因素，实现起来较为复杂。

![img](https://developer.ibm.com/developer/articles/ba-cn-apache-spark-memory-management/nl/zh/images/image006.png)

### 统一内存管理参数

```
参数：spark.memory.fraction
含义：用于执行和存储的部分（堆空间-300MB）。值越低，溢出和缓存数据逐出的频率就越高。此配置的目的是为稀疏的，异常大的记录留出用于内部元数据，用户数据结构和大小估计不精确的内存。建议将其保留为默认值。
默认值：0.6 (Spark 1.6 默认为 0.75，Spark 2.0+ 默认为 0.6)

参数：spark.memory.storageFraction
含义：可以逐出的存储内存量，以spark.memory.fraction预留的区域大小的一部分表示。数值越高，则可用于执行的工作内存就越少，任务可能会更频繁地溢出到磁盘上. 建议将其保留为默认值。
默认值：0.5

参数：spark.memory.offHeap.enabled
含义：如果为true，Spark将尝试将堆外内存用于某些操作。如果启用了堆外内存使用，则spark.memory.offHeap.size必须为正。
默认值：false

参数：spark.memory.offHeap.size
含义：可用于堆外分配的绝对内存量（以字节为单位）。此设置对堆内存使用没有影响，因此，如果执行者的总内存消耗必须在某个硬限制内，那么请确保相应地缩小JVM堆大小。当spark.memory.offHeap.enabled=true时，必须将此值设置为正值.
默认值：0

参数：spark.memory.useLegacyMode 
含义：是否启用Spark 1.5及之前版本中使用的旧版内存管理模式。传统模式将堆空间严格划分为固定大小的区域，如果未调整应用程序，则可能导致过多的溢出. 除非已启用，否则不会读取以下不推荐使用的内存分数配置： spark.shuffle.memoryFraction、spark.storage.memoryFraction、spark.storage.unrollFraction
默认值：false

参数：spark.shuffle.memoryFraction
含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。在 shuffle 期间用于聚合和协同分组的Java堆的分数。在任何给定时间，用于随机播放的所有内存映射的集合大小都受到此限制的限制，超出此限制，内容将开始溢出到磁盘。如果经常发生泄漏，请考虑以spark.storage.memoryFraction为代价增加此值.
默认值：0.2

参数：spark.storage.memoryFraction
含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。用于Spark的内存缓存的Java堆的分数。这不应大于JVM中对象的"旧"代，默认情况下，该对象的堆大小为0.6，但是如果您配置自己的旧代大小，则可以增加它。
默认值：0.6

参数：spark.storage.unrollFraction
含义：（不建议使用）仅在启用spark.memory.useLegacyMode读。spark.storage.memoryFraction分数，用于展开内存中的块。当没有足够的可用存储空间来完全展开新块时，通过删除现有块来动态分配该块。
默认值：0.2
```

### 内存参数示例

为了方便理解堆内内存，可以通过一个简单的例子来看看。假设我们提交的 Spark 作业内存设置为 --executor-memory 18g。

```shell
spark-submit \
--master yarn \
--deploy-mode client \
--executor-memory 18g \
--queue root.exquery \
--class org.apache.spark.examples.SparkPi \
/opt/cloudera/parcels/CDH/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.14.4-hadoop2.6.0-cdh5.14.4.jar \
100  
```

Spark UI 页面显示的内存情况：

![img](https://img2020.cnblogs.com/blog/1372882/202008/1372882-20200804152614883-1618856975.png)

可以看出，Storage Memory 的可用内存是 10 GB，这个数咋来的呢？根据前面的内存规则，可以得出以下计算：

```
systemMemory = spark.executor.memory
reservedMemory = 300MB
usableMemory = systemMemory - reservedMemory
StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction
```

代入数据，可以得出以下结果：

```
systemMemory = 18Gb = 19327352832 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032
StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction
             = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB
```

结果和 Spark UI 界面展示的 10GB 差不多有一般的差距，这是因为 Spark UI 上显示的 Storage Memory 可用内存其实等于 Storage 内存和 Execution 内存之和，也就是 usableMemory * spark.memory.fraction。

```
StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction
             = 19012780032 * 0.6 / （1024 * 1024 * 1024) = 10.62421 GB
```

这个结果和 Spark UI 上看到的结果还是有点出入，为什么呢？这是因为虽然我们设置了 --executor-memory 18g，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory = 17179869184，然后计算的数据如下：

```
systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory= usableMemory * spark.memory.fraction
             = 16865296384 * 0.6 /（1024 * 1024 * 1024)  = 9.42421 GB
```

我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：

```
systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory= usableMemory * spark.memory.fraction
             = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB
```

systemMemory 内存之所以为 17179869184（可以通过方法 Runtime.getRuntime.maxMemory( ) 获得该值），是因为内存分配池的堆部分分为 Eden，Survivor 和 Tenured 三部分空间，而这里面一共包含了两个 Survivor 区域，而这两个 Survivor 区域在任何时候我们只能用到其中一个，所以我们可以使用下面的公式进行描述：

```
ExecutorMemory = Eden + 2 * Survivor + Tenured
Runtime.getRuntime.maxMemory =  Eden + Survivor + Tenured
```

上面的 17179869184 字节可能因为 GC 配置不一样得到的数据不一样，但是上面的计算公式是一样的。

参考：

https://www.cnblogs.com/ronnieyuan/p/11742974.html

https://www.cnblogs.com/lemonu/p/13433388.html

## SparkSQL的执行计划详解

https://www.cnblogs.com/lyr999736/p/10204619.html

## Spark常见错误

[数据倾斜，excutor严重GC，ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM](http://blog.sina.com.cn/s/blog_628cc2b70102xctj.html)

[spark 读取ORC文件时间太长（计算Partition时间太长）且产出orc单个文件中stripe个数太多](https://blog.csdn.net/aijiudu/article/details/78616064)

https://www.cnblogs.com/double-kill/p/9012383.html

https://blog.csdn.net/lsshlsw/article/details/49155087

https://blog.csdn.net/lingbo229/article/details/100879126
https://www.cnblogs.com/zz-ksw/p/11403622.html

[华为故障处理参考](https://support.huawei.com/enterprise/zh/doc/EDOC1000175674?section=j005)

[spark sql插入表时的文件个数研究](https://www.cnblogs.com/itboys/p/11076874.html)

[Spark Shuffle实现原理及代码解析](https://www.cnblogs.com/barneywill/p/10158457.html)

[Spark Failures 失败task 类型](https://www.jianshu.com/p/aa038adbbbfb)

https://www.itdiandi.net/view/863
https://blog.csdn.net/jeffiny/article/details/80105183

[ERROR storage.DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file](https://www.cnblogs.com/LBSer/p/4129481.html)

# Flink

## Flink  API

## 窗口机制

[Flink 原理与实现：Window 机制](http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/)

### 窗口概念

**窗口通过周期性的规则(基于时间、数量等)将无界数据量逻辑划分为一批批的有界数据集合，可以认为窗口是流到批的一个桥梁**

### 思考窗口的生命周期

通过以上的内容，我们应该知道了窗口的作用(主要是为了解决什么样的问题)。那么这个时候需要思考四个问题

1. 数据元素是如何分配到对应窗口中的(也就是窗口的分配器)？
2. 元素分配到对应窗口之后什么时候会触发计算(也就是窗口的触发器)？
3. 在窗口内我们能够进行什么样的操作(也就是窗口内的操作)？
4. 当窗口过期后是如何处理的(也就是窗口的销毁关闭)？

其实这四个问题从大体上可以理解为窗口的整个生命周期过程。接下来我们对每个环节进行讲解

### 自带三种window

Flink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window：

- 以时间驱动的 Time Window
- 以事件数量驱动的 Count Window
- 以会话间隔驱动的 Session Window

DataStream通过调用keyBy转换成KedyedStream，再经过window转换成WindowedStream,然后再基于WindowedStream进行reduce、aggregate或者process等窗口函数进行操作

![image-20210315160320878](https://tva1.sinaimg.cn/large/e6c9d24ely1gokngf8lf9j20iw07zwhs.jpg)

#### Time Window 使用及源码分析

在 Flink 中使用 Time Window 非常简单，输入一个时间参数，这个时间参数可以利用 Time 这个类来控制，如果事前没指定 TimeCharacteristic 类型的话，则默认使用的是 ProcessingTime.

```java
dataStream.keyBy(1)
    .timeWindow(Time.minutes(1)) //time Window 每分钟统计一次数量和
    .sum(1);
```

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko38lvqvj21o70u0wgo.jpg)

该 timeWindow 方法在 KeyedStream 中对应的源码如下：

```java
//时间窗口
public WindowedStream<T, KEY, TimeWindow> timeWindow(Time size) {
    if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) {
        return window(TumblingProcessingTimeWindows.of(size));
    } else {
        return window(TumblingEventTimeWindows.of(size));
    }
}
```

另外在 Time Window 中还支持滑动的时间窗口，比如定义了一个每 30s 滑动一次的 1 分钟时间窗口，它会每隔 30s 去统计过去一分钟窗口内的数据，同样使用也很简单，输入两个时间参数，如下：

```java
dataStream.keyBy(1)
    .timeWindow(Time.minutes(1), Time.seconds(30)) //sliding time Window 每隔 30s 统计过去一分钟的数量和
    .sum(1);
```

滑动时间窗口的数据聚合流程如下图所示：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko39ljj8j21l70u0410.jpg)

在该第一个时间窗口中（1 ～ 2 分钟）和为 7，第二个时间窗口中（1:30 ~ 2:30）和为 10，第三个时间窗口中（2 ~ 3 分钟）和为 12，第四个时间窗口中（2:30 ~ 3:30）和为 10，第五个时间窗口中（3 ~ 4 分钟）和为 7，第六个时间窗口中（3:30 ~ 4:30）和为 11，第七个时间窗口中（4 ~ 5 分钟）和为 19。

该 timeWindow 方法在 KeyedStream 中对应的源码如下：

```java
//滑动时间窗口
public WindowedStream<T, KEY, TimeWindow> timeWindow(Time size, Time slide) {
    if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) {
        return window(SlidingProcessingTimeWindows.of(size, slide));
    } else {
        return window(SlidingEventTimeWindows.of(size, slide));
    }
}
```

#### Count Window 使用及源码分析

Apache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个元素时才会计算窗口中所有事件的值。

在 Flink 中使用 Count Window 非常简单，输入一个 long 类型的参数，这个参数代表窗口中事件的数量，使用如下：

```java
dataStream.keyBy(1)
    .countWindow(3) //统计每 3 个元素的数量之和
    .sum(1);
```

计数窗口的数据窗口聚合流程如下图所示：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3docc7j21ek0ny769.jpg)

该 countWindow 方法在 KeyedStream 中对应的源码如下：

```java
//计数窗口
public WindowedStream<T, KEY, GlobalWindow> countWindow(long size) {
    return window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));
}
```

另外在 Count Window 中还支持滑动的计数窗口，比如定义了一个每 3 个事件滑动一次的 4 个事件的计数窗口，它会每隔 3 个事件去统计过去 4 个事件计数窗口内的数据，使用也很简单，输入两个 long 类型的参数，如下：

```java
dataStream.keyBy(1) 
    .countWindow(4, 3) //每隔 3 个元素统计过去 4 个元素的数量之和
    .sum(1);
```

滑动计数窗口的数据窗口聚合流程如下图所示：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3hkwrqj21ze0u0ac7.jpg)

该 countWindow 方法在 KeyedStream 中对应的源码如下：

```java
//滑动计数窗口
public WindowedStream<T, KEY, GlobalWindow> countWindow(long size, long slide) {
    return window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));
}
```

#### Session Window 使用及源码分析

Apache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。

在 Flink 中使用 Session Window 非常简单，你该使用 Flink KeyedStream 中的 window 方法，然后使用 ProcessingTimeSessionWindows.withGap()（不一定就是只使用这个），在该方法里面你需要做的是传入一个时间参数，如下：

```java
dataStream.keyBy(1)
    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和
    .sum(1);
```

会话窗口的数据窗口聚合流程如下图所示：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3kt8nsj22830u0q5w.jpg)

该 Window 方法在 KeyedStream 中对应的源码如下：

```java
//提供自定义 Window
public <W extends Window> WindowedStream<T, KEY, W> window(WindowAssigner<? super T, W> assigner) {
    return new WindowedStream<>(this, assigner);
}
```

### 如何自定义 Window，窗口的剔除器, 触发器以及选择器

三个概念：WindowAssigner(窗口分配器)、Trigger(触发器)、Evictor(剔除器)

当然除了上面几种自带的 Window 外，Apache Flink 还提供了用户可自定义的 Window，那么该如何操作呢？其实细心的同学可能已经发现了上面我写的每种 Window 的实现方式了，它们有 assigner、 evictor、trigger。如果你没发现的话，也不要紧，这里我们就来了解一下实现 Window 的机制，这样我们才能够更好的学会如何自定义 Window。

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko43z9vzj219z0u0jtj.jpg)

#### Window 源码定义

上面说了 Flink 中自带的 Window，主要利用了 KeyedStream 的 API 来实现，我们这里来看下 Window 的源码定义如下：

```java
public abstract class Window {
    //获取属于此窗口的最大时间戳
    public abstract long maxTimestamp();
}
```

查看源码可以看见 Window 这个抽象类有如下实现类：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3ozyawj20y20as3yg.jpg)

**TimeWindow** 源码定义如下:

```java
public class TimeWindow extends Window {
    //窗口开始时间
    private final long start;
    //窗口结束时间
    private final long end;
}
```

**GlobalWindow** 源码定义如下：

```java
public class GlobalWindow extends Window {

    private static final GlobalWindow INSTANCE = new GlobalWindow();

    private GlobalWindow() { }
    //对外提供 get() 方法返回 GlobalWindow 实例，并且是个全局单例
    public static GlobalWindow get() {
        return INSTANCE;
    }
}
```

#### Window 组件之 WindowAssigner 使用及源码分析

到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。

窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。我们来看下 WindowAssigner 的代码的定义吧：

```java
public abstract class WindowAssigner<T, W extends Window> implements Serializable {
    //分配数据到窗口并返回窗口集合
    public abstract Collection<W> assignWindows(T element, long timestamp, WindowAssignerContext context);
}
```

查看源码可以看见 WindowAssigner 这个抽象类有如下实现类：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3r8cstj22ro0p0wey.jpg)

这些 WindowAssigner 实现类的作用介绍：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-155715.jpg)

如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，比如我拿 TumblingEventTimeWindows 的源码来分析，如下：

```java
public class TumblingEventTimeWindows extends WindowAssigner<Object, TimeWindow> {
    //定义属性
    private final long size;
    private final long offset;

    //构造方法
    protected TumblingEventTimeWindows(long size, long offset) {
        if (Math.abs(offset) >= size) {
            throw new IllegalArgumentException("TumblingEventTimeWindows parameters must satisfy abs(offset) < size");
        }
        this.size = size;
        this.offset = offset;
    }

    //重写 WindowAssigner 抽象类中的抽象方法 assignWindows
    @Override
    public Collection<TimeWindow> assignWindows(Object element, long timestamp, WindowAssignerContext context) {
        //实现该 TumblingEventTimeWindows 中的具体逻辑
    }

    //其他方法，对外提供静态方法，供其他类调用
}
```

从上面你就会发现**套路**：

1、定义好实现类的属性

2、根据定义的属性添加构造方法

3、重写 WindowAssigner 中的 assignWindows 等方法

4、定义其他的方法供外部调用

#### Window 组件之 Trigger 使用及源码分析

Trigger 表示触发器，**每个窗口都拥有一个 Trigger（触发器），该 Trigger 决定何时计算和清除窗口。**当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发，即清除（删除窗口并丢弃其内容），或者启动并清除窗口。**一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。**

说了这么一大段，我们还是来看看 Trigger 的源码，定义如下：

```java
public abstract class Trigger<T, W extends Window> implements Serializable {
    //当有数据进入到 Window 运算符就会触发该方法
    public abstract TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx) throws Exception;
    //当使用触发器上下文设置的处理时间计时器触发时调用
    public abstract TriggerResult onProcessingTime(long time, W window, TriggerContext ctx) throws Exception;
    //当使用触发器上下文设置的事件时间计时器触发时调用该方法
    public abstract TriggerResult onEventTime(long time, W window, TriggerContext ctx) throws Exception;
}

Trigger接口的五种方法：
onElement()方法：当某个窗口增加一个元素时，会调用该方法，返回一个TriggerResult
onEventTime()方法：当一个基于Event Time的Timer触发了FIRE时调用onEventTime方法
onProcessingTime()方法：当一个基于Processing Time的Timer触发了FIRE时调用onProcessTime方法
onMerge()方法：和有状态的触发器有关，当多个窗口被合并时调用onMerge，并会合并触发器的状态，例如使用会话窗口时。
clear()方法：当窗口数据被清理时，调用clear方法来清理所有的Trigger状态数据，否则随着窗口越来越多，状态数据也会越来越多
```

当有数据流入 Window 运算符时就会触发 onElement 方法、当处理时间和事件时间生效时会触发 onProcessingTime 和 onEventTime 方法。每个触发动作的返回结果用 TriggerResult 定义。继续来看下 TriggerResult 的源码定义：

```java
public enum TriggerResult {

    //不做任何操作
    CONTINUE(false, false),

    //处理并移除窗口中的数据
    FIRE_AND_PURGE(true, true),

    //处理窗口数据，窗口计算后不做清理
    FIRE(true, false),

    //清除窗口中的所有元素，并且在不计算窗口函数或不发出任何元素的情况下丢弃窗口
    PURGE(false, true);
}
```

查看源码可以看见 Trigger 这个抽象类有如下实现类：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163751.png)

这些 Trigger 实现类的作用介绍：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1goko3za2buj21i50u0dnt.jpg)

如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，拿 CountTrigger 的源码来分析，如下：

```java
public class CountTrigger<W extends Window> extends Trigger<Object, W> {
    //定义属性
    private final long maxCount;

    private final ReducingStateDescriptor<Long> stateDesc = new ReducingStateDescriptor<>("count", new Sum(), LongSerializer.INSTANCE);
    //构造方法
    private CountTrigger(long maxCount) {
        this.maxCount = maxCount;
    }

    //重写抽象类 Trigger 中的抽象方法 
    @Override
    public TriggerResult onElement(Object element, long timestamp, W window, TriggerContext ctx) throws Exception {
        //实现 CountTrigger 中的具体逻辑
    }

    @Override
    public TriggerResult onEventTime(long time, W window, TriggerContext ctx) {
        return TriggerResult.CONTINUE;
    }

    @Override
    public TriggerResult onProcessingTime(long time, W window, TriggerContext ctx) throws Exception {
        return TriggerResult.CONTINUE;
    }
}
```

**套路**：

1. 定义好实现类的属性
2. 根据定义的属性添加构造方法
3. 重写 Trigger 中的 onElement、onEventTime、onProcessingTime 等方法
4. 定义其他的方法供外部调用

#### Window 组件之 Evictor 使用及源码分析

Evictor 表示驱逐者，它可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素，然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。

我们来看看 Evictor 的源码定义如下：

```java
public interface Evictor<T, W extends Window> extends Serializable {
    //在窗口函数之前调用该方法选择性地清除元素
    void evictBefore(Iterable<TimestampedValue<T>> elements, int size, W window, EvictorContext evictorContext);
    //在窗口函数之后调用该方法选择性地清除元素
    void evictAfter(Iterable<TimestampedValue<T>> elements, int size, W window, EvictorContext evictorContext);
}
```

查看源码可以看见 Evictor 这个接口有如下实现类：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163942.png)

这些 Evictor 实现类的作用介绍：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-17-153505.jpg)

如果你细看了上面三种中某个类的实现的话，你会发现一个规律，比如我就拿 CountEvictor 的源码来分析，如下：

```java
public class CountEvictor<W extends Window> implements Evictor<Object, W> {
    private static final long serialVersionUID = 1L;

    //定义属性
    private final long maxCount;
    private final boolean doEvictAfter;

    //构造方法
    private CountEvictor(long count, boolean doEvictAfter) {
        this.maxCount = count;
        this.doEvictAfter = doEvictAfter;
    }
    //构造方法
    private CountEvictor(long count) {
        this.maxCount = count;
        this.doEvictAfter = false;
    }

    //重写 Evictor 中的 evictBefore 方法
    @Override
    public void evictBefore(Iterable<TimestampedValue<Object>> elements, int size, W window, EvictorContext ctx) {
        if (!doEvictAfter) {
            //调用内部的关键实现方法 evict
            evict(elements, size, ctx);
        }
    }

    //重写 Evictor 中的 evictAfter 方法
    @Override
    public void evictAfter(Iterable<TimestampedValue<Object>> elements, int size, W window, EvictorContext ctx) {
        if (doEvictAfter) {
            //调用内部的关键实现方法 evict
            evict(elements, size, ctx);
        }
    }

    private void evict(Iterable<TimestampedValue<Object>> elements, int size, EvictorContext ctx) {
        //内部的关键实现方法
    }

    //其他的方法
}
```

发现**套路**：

1. 定义好实现类的属性
2. 根据定义的属性添加构造方法
3. 重写 Evictor 中的 evictBefore 和 evictAfter 方法
4. 定义关键的内部实现方法 evict，处理具体的逻辑
5. 定义其他的方法供外部调用

#### 自定义 Window？

通过这几个源码，我们可以发现，它最后调用的都有一个方法，那就是 Window 方法，如下：

```java
//提供自定义 Window
public <W extends Window> WindowedStream<T, KEY, W> window(WindowAssigner<? super T, W> assigner) {
    return new WindowedStream<>(this, assigner);
}

//构造一个 WindowedStream 实例
public WindowedStream(KeyedStream<T, K> input,
        WindowAssigner<? super T, W> windowAssigner) {
    this.input = input;
    this.windowAssigner = windowAssigner;
    //获取一个默认的 Trigger
    this.trigger = windowAssigner.getDefaultTrigger(input.getExecutionEnvironment());
}
```

可以看到这个 Window 方法传入的参数是一个 WindowAssigner 对象（你可以利用 Flink 现有的 WindowAssigner，也可以根据上面的方法来自定义自己的 WindowAssigner），然后再通过构造一个 WindowedStream 实例（在构造实例的会传入 WindowAssigner 和获取默认的 Trigger）来创建一个 Window。

另外你可以看到滑动计数窗口，在调用 window 方法之后，还调用了 WindowedStream 的 evictor 和 trigger 方法，trigger 方法会覆盖掉你之前调用 Window 方法中默认的 trigger，如下：

```java
//滑动计数窗口
public WindowedStream<T, KEY, GlobalWindow> countWindow(long size, long slide) {
    return window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));
}

//trigger 方法
public WindowedStream<T, K, W> trigger(Trigger<? super T, ? super W> trigger) {
    if (windowAssigner instanceof MergingWindowAssigner && !trigger.canMerge()) {
        throw new UnsupportedOperationException("A merging window assigner cannot be used with a trigger that does not support merging.");
    }

    if (windowAssigner instanceof BaseAlignedWindowAssigner) {
        throw new UnsupportedOperationException("Cannot use a " + windowAssigner.getClass().getSimpleName() + " with a custom trigger.");
    }
    //覆盖之前的 trigger
    this.trigger = trigger;
    return this;
}
```

从上面的各种窗口实现，你就会发现了：Evictor 是可选的，但是 WindowAssigner 和 Trigger 是必须会有的，这种创建 Window 的方法充分利用了 KeyedStream 和 WindowedStream 的 API，再加上现有的 WindowAssigner、Trigger、Evictor，你就可以创建 Window 了，另外你还可以自定义这三个窗口组件的实现类来满足你公司项目的需求。

## Watermark

当人们第一次使用 Flink 时，经常会对 watermark 感到困惑。但其实 watermark 并不复杂。让我们通过一个简单的例子来说明为什么我们需要 watermark，以及它的工作机制是什么样的。

### 在 Apache Flink 中使用 watermark 的 4 个理解

在下文中的例子中，我们有一个带有时间戳的事件流，但是由于某种原因它们并不是按顺序到达的。图中的数字代表事件发生的时间戳。第一个到达的事件发生在时间 4，然后它后面跟着的是发生在更早时间（时间 2）的事件，以此类推：

[![img](https://img.alicdn.com/tfs/TB1W0UdqcfpK1RjSZFOXXa6nFXa-684-133.png)](https://img.alicdn.com/tfs/TB1W0UdqcfpK1RjSZFOXXa6nFXa-684-133.png)

注意这是一个按照事件时间处理的例子，这意味着时间戳反映的是事件发生的时间，而不是处理事件的时间。事件时间（Event-Time）处理的强大之处在于，无论是在处理实时的数据还是重新处理历史的数据，基于事件时间创建的流计算应用都能保证结果是一样的。

*注：可以访问 Apache Flink 文档，了解更多有关时间的概念，如 event-time, processing-time, ingestion-time。*

现在假设我们正在尝试创建一个流计算排序算子。也就是处理一个乱序到达的事件流，并按照事件时间的顺序输出事件。

##### 理解 #1:

数据流中的第一个元素的时间是 4，但是我们不能直接将它作为排序后数据流的第一个元素并输出它。因为数据是乱序到达的，也许有一个更早发生的数据还没有到达。事实上，我们能预见一些这个流的未来，也就是我们的排序算子至少要等到 2 这条数据的到达再输出结果。

**有缓存，就必然有延迟。**

##### 理解 #2:

如果我们做错了，我们可能会永远等待下去。首先，我们的应用程序从看到时间 4 的数据，然后看到时间 2 的数据。是否会有一个比时间 2 更早的数据到达呢？也许会，也许不会。我们可以一直等下去，但可能永远看不到 1 。

**最终，我们必须勇敢地输出 2 作为排序流的第一个结果。**

##### 理解 #3:

我们需要的是某种策略，它定义了对于任何带时间戳的事件流，何时停止等待更早数据的到来。

**这正是 watermark 的作用，他们定义了何时不再等待更早的数据。**

Flink 中的事件时间处理依赖于一种特殊的带时间戳的元素，成为 watermark，它们会由数据源或是 watermark 生成器插入数据流中。具有时间戳 `t` 的 watermark 可以被理解为断言了所有时间戳**小于或等于** `t` 的事件都（在某种合理的概率上）已经到达了。

> 译注：此处原文是“小于”，译者认为应该是 “小于或等于”，因为 Flink 源码中采用的是 “小于或等于” 的机制。

何时我们的排序算子应该停止等待，然后将事件 2 作为首个元素输出？答案是当收到时间戳为 2（或更大）的 watermark 时。

##### 理解 #4:

**我们可以设想不同的策略来生成 watermark。**

我们知道每个事件都会延迟一段时间才到达，而这些延迟差异会比较大，所以有些事件会比其他事件延迟更多。一种简单的方法是假设这些延迟不会超过某个最大值。Flink 把这种策略称作 “有界无序生成策略”（bounded-out-of-orderness）。当然也有很多更复杂的方式去生成 watermark，但是对于大多数应用来说，固定延迟的方式已经足够了。

如果想要构建一个类似排序的流应用，可以使用 Flink 的 `ProcessFunction`。它提供了对事件时间计时器（基于 watermark 触发回调）的访问，还提供了可以用来缓存数据的托管状态接口。

## Flink WaterMark 详解及结合 WaterMark 处理延迟数据

在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 & 事件延迟。

下图表示选择 Event Time 与 Process Time 的实际效果图：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-21-140842.jpg)

在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。比如下图：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-21-152340.jpg)

然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：

- 错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首
- 设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件

这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 & 事件延迟），Flink 引入了 Watermark 机制来解决。

### Watermark 是什么？

举个例子：

统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。

Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。

下面通过几个图来了解一下 Watermark 是如何工作的！

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154340.jpg)

上图中的数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，`W(4)` 和 `W(9)` 是水印。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154747.jpg)

经过 Flink 的消费，数据 `1`、`3`、`2` 进入了第一个窗口，然后 `7` 会进入第二个窗口，接着 `3` 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 `4` 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155309.jpg)

那么接着后面的数据 `5` 和 `6` 会进入到第二个窗口里面，数据 `9` 会进入在第三个窗口里面。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155558.jpg)

那么当遇到水印 `9` 时，发现水印比第二个窗口的结束时间 `8` 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去。

相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。

### Flink 中 Watermark 的设置

在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。

```java
public SingleOutputStreamOperator<T> assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks<T> timestampAndWatermarkAssigner) {

    final int inputParallelism = getTransformation().getParallelism();
    final AssignerWithPeriodicWatermarks<T> cleanedAssigner = clean(timestampAndWatermarkAssigner);

    TimestampsAndPeriodicWatermarksOperator<T> operator = new TimestampsAndPeriodicWatermarksOperator<>(cleanedAssigner);

    return transform("Timestamps/Watermarks", getTransformation().getOutputType(), operator).setParallelism(inputParallelism);
}

public SingleOutputStreamOperator<T> assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks<T> timestampAndWatermarkAssigner) {

    final int inputParallelism = getTransformation().getParallelism();
    final AssignerWithPunctuatedWatermarks<T> cleanedAssigner = clean(timestampAndWatermarkAssigner);

    TimestampsAndPunctuatedWatermarksOperator<T> operator = new TimestampsAndPunctuatedWatermarksOperator<>(cleanedAssigner);

    return transform("Timestamps/Watermarks", getTransformation().getOutputType(), operator).setParallelism(inputParallelism);
}

复制
```

所以设置 Watermark 是有如下两种方式：

- AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。

在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。

- AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。

在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。

下面再分别详细讲下这两种的实现方式。

### Punctuated Watermark

AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。

那么该怎么利用这个来定义水印生成器呢？

```java
public class WordPunctuatedWatermark implements AssignerWithPunctuatedWatermarks<Word> {

    @Nullable
    @Override
    public Watermark checkAndGetNextWatermark(Word lastElement, long extractedTimestamp) {
        return extractedTimestamp % 3 == 0 ? new Watermark(extractedTimestamp) : null;
    }

    @Override
    public long extractTimestamp(Word element, long previousElementTimestamp) {
        return element.getTimestamp();
    }
}

复制
```

需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。

### Periodic Watermark

通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。

```java
public class WordWatermark implements AssignerWithPeriodicWatermarks<Word> {

    private long currentTimestamp = Long.MIN_VALUE;

    @Override
    public long extractTimestamp(Word word, long previousElementTimestamp) {
        if (word.getTimestamp() > currentTimestamp) {
            this.currentTimestamp = word.getTimestamp();
        }
        return currentTimestamp;
    }

    @Nullable
    @Override
    public Watermark getCurrentWatermark() {
        long maxTimeLag = 5000;
        return new Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);

    }
}

复制
```

上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，该方法会返回当前时间戳与事件时间进行比较，如果事件的时间戳比 currentTimestamp 大的话，那么就将当前事件的时间戳赋值给 currentTimestamp。getCurrentWatermark() 方法是获取当前的水位线，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 `long maxTimeLag = 5000;` 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。

通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。

AssignerWithPeriodicWatermarks 这个接口有四个实现类，分别如下图：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082804.png)

- BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083043.png)

你可以像下面一样使用该类来分配时间和生成水印：

```java
//Time.seconds(10) 代表允许延迟的时间大小
dataStream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<Event>(Time.seconds(10)) {
    //重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法
    @Override
    public long extractTimestamp(Event event) {
        return event.getTimestamp();
    }
})

复制
```

- CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。
- AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：

```java
public final long extractTimestamp(T element, long elementPrevTimestamp) {
    final long newTimestamp = extractAscendingTimestamp(element);
    if (newTimestamp >= this.currentTimestamp) {
        this.currentTimestamp = ne∏wTimestamp;
        return newTimestamp;
    } else {
        violationHandler.handleViolation(newTimestamp, this.currentTimestamp);
        return newTimestamp;
    }
}

复制
```

- IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 `System.currentTimeMillis()` 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。

**注意**：

1、使用这种方式周期性生成水印的话，你可以通过 `env.getConfig().setAutoWatermarkInterval(...);` 来设置生成水印的间隔（每隔 n 毫秒）。

2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义

```java
@Override
public void run(SourceContext<MyType> ctx) throws Exception {
    while (/* condition */) {
        MyType next = getNext();
        ctx.collectWithTimestamp(next, next.getEventTimestamp());

        if (next.hasWatermarkTime()) {
            ctx.emitWatermark(new Watermark(next.getWatermarkTime()));
        }
    }
}

复制
```

### 每个 Kafka 分区的时间戳

当以 Kafka 来作为数据源的时候，通常每个 Kafka 分区的数据时间戳是递增的（事件是有序的），但是当你作业设置多个并行度的时候，Flink 去消费 Kafka 数据流是并行的，那么并行的去消费 Kafka 分区的数据就会导致打乱原每个分区的数据时间戳的顺序。在这种情况下，你可以使用 Flink 中的 `Kafka-partition-aware` 特性来生成水印，使用该特性后，水印会在 Kafka 消费端生成，然后每个 Kafka 分区和每个分区上的水印最后的合并方式和水印在数据流 shuffle 过程中的合并方式一致。

如果事件时间戳严格按照每个 Kafka 分区升序，则可以使用前面提到的 AscendingTimestampExtractor 水印生成器来为每个分区生成水印。下面代码教大家如何使用 `per-Kafka-partition` 来生成水印。

```java
FlinkKafkaConsumer011<Event> kafkaSource = new FlinkKafkaConsumer011<>("zhisheng", schema, props);
kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor<Event>() {

    @Override
    public long extractAscendingTimestamp(Event event) {
        return event.eventTimestamp();
    }
});

DataStream<Event> stream = env.addSource(kafkaSource);

复制
```

下图表示水印在 Kafka 分区后如何通过流数据流传播：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-09-014107.jpg)

### Watermark 与 Window 结合来处理延迟数据

其实在上文中已经提到的一点是在设置 Periodic Watermark 时，是允许提供一个参数，表示数据最大的延迟时间。其实这个值要结合自己的业务以及数据的情况来设置，如果该值设置的太小会导致数据因为网络或者其他的原因从而导致乱序或者延迟的数据太多，那么最后窗口触发的时候，可能窗口里面的数据量很少，那么这样计算的结果很可能误差会很大，对于有的场景（要求正确性比较高）是不太符合需求的。但是如果该值设置的太大，那么就会导致很多窗口一直在等待延迟的数据，从而一直不触发，这样首先就会导致数据的实时性降低，另外将这么多窗口的数据存在内存中，也会增加作业的内存消耗，从而可能会导致作业发生 OOM 的问题。

综上建议：

- 合理设置允许数据最大延迟时间
- 不太依赖事件时间的场景就不要设置时间策略为 EventTime

### 延迟数据该如何处理(三种方法)

#### 丢弃（默认）

在 Flink 中，对这么延迟数据的默认处理方式是丢弃。

#### allowedLateness 再次指定允许数据延迟的时间

allowedLateness 表示允许数据延迟的时间，这个方法是在 WindowedStream 中的，用来设置允许窗口数据延迟的时间，超过这个时间的元素就会被丢弃，这个的默认值是 0，该设置仅针对于以事件时间开的窗口，它的源码如下：

```java
public WindowedStream<T, K, W> allowedLateness(Time lateness) {
    final long millis = lateness.toMilliseconds();
    checkArgument(millis >= 0, "The allowed lateness cannot be negative.");

    this.allowedLateness = millis;
    return this;
}

复制
```

之前有多个小伙伴问过我 Watermark 中允许的数据延迟和这个数据延迟的区别是啥？我的回复是该允许延迟的时间是在 Watermark 允许延迟的基础上增加的时间。那么具体该如何使用 allowedLateness 呢。

```java
dataStream.assignTimestampsAndWatermarks(new TestWatermarkAssigner())
    .keyBy(new TestKeySelector())
    .timeWindow(Time.milliseconds(1), Time.milliseconds(1))
    .allowedLateness(Time.milliseconds(2))  //表示允许再次延迟 2 毫秒
    .apply(new WindowFunction<Integer, String, Integer, TimeWindow>() {
        //计算逻辑
    });

复制
```

#### sideOutputLateData 收集迟到的数据

sideOutputLateData 这个方法同样是 WindowedStream 中的方法，该方法会将延迟的数据发送到给定 OutputTag 的 side output 中去，然后你可以通过 `SingleOutputStreamOperator.getSideOutput(OutputTag)` 来获取这些延迟的数据。具体的操作方法如下：

```java
//定义 OutputTag
OutputTag<Integer> lateDataTag = new OutputTag<Integer>("late"){};

SingleOutputStreamOperator<String> windowOperator = dataStream
        .assignTimestampsAndWatermarks(new TestWatermarkAssigner())
        .keyBy(new TestKeySelector())
        .timeWindow(Time.milliseconds(1), Time.milliseconds(1))
        .allowedLateness(Time.milliseconds(2))
        .sideOutputLateData(lateDataTag)    //指定 OutputTag
        .apply(new WindowFunction<Integer, String, Integer, TimeWindow>() {
            //计算逻辑
        });

windowOperator.addSink(resultSink);

//通过指定的 OutputTag 从 Side Output 中获取到延迟的数据之后，你可以通过 addSink() 方法存储下来，这样可以方便你后面去排查哪些数据是延迟的。
windowOperator.getSideOutput(lateDataTag)
        .addSink(lateResultSink);

复制
```

### 小结与反思

本节讲了 Watermark 的概念，并讲解了 Flink 中自带的 Watermark，然后还教大家如何设置 Watermark 以及如何自定义 Watermark，最后通过结合 Window 与 Watermark 去处理延迟数据，还讲解了三种常见的处理延迟数据的方法。

关于 Watermark 你有遇到什么问题吗？对于延迟数据你通常是怎么处理的？

本节相关的代码地址：[Watermark](https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/watermark)



## TTL、State；

## Flink State 深度讲解

在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。

### 为什么需要 state？

对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：

- Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费
- Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1golw7pn464j21k20qg752.jpg)

为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录这种数据消费状态，Flink state 就这样诞生了，**state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态）**，当 Job 因为某种错误或者其他原因导致重启时，就能够从 checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。

### State 的种类

在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。

### Keyed State

Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。

### Operator State

对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。

当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。

在 Flink 源码中，在 flink-core module 下的 org.apache.flink.api.common.state 中可以看到 Flink 中所有和 State 相关的类。

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1golwgqox05j20u014fq4n.jpg)

### Raw and Managed State

Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。

原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 checkpoint 时，原始状态会以字节流的形式写入进 checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。

托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 checkpoint 中。

DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。

注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。

### 如何使用托管 Keyed State

托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。

我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：

- ValueState: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。
- ListState: 保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。
- ReducingState: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。
- AggregatingState: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。
- FoldingState: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。
- MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。

所有类型的状态都有一个 clear() 方法来清除当前的状态。

注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。

需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。

要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。

状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：

- ValueState getState(ValueStateDescriptor)
- ReducingState getReducingState(ReducingStateDescriptor)
- ListState getListState(ListStateDescriptor)
- AggregatingState getAggregatingState(AggregatingState)
- FoldingState getFoldingState(FoldingStateDescriptor)
- MapState getMapState(MapStateDescriptor)

上面讲了这么多概念，那么来一个例子来看看如何使用状态：

```java
public class CountWindowAverage extends RichFlatMapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>> {

    //ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 
    private transient ValueState<Tuple2<Long, Long>> sum;

    @Override
    public void flatMap(Tuple2<Long, Long> input, Collector<Tuple2<Long, Long>> out) throws Exception {

        //访问状态的 value 值
        Tuple2<Long, Long> currentSum = sum.value();

        //更新 count
        currentSum.f0 += 1;

        //更新 sum
        currentSum.f1 += input.f1;

        //更新状态
        sum.update(currentSum);

        //如果 count 等于 2, 发出平均值并清除状态
        if (currentSum.f0 >= 2) {
            out.collect(new Tuple2<>(input.f0, currentSum.f1 / currentSum.f0));
            sum.clear();
        }
    }

    @Override
    public void open(Configuration config) {
        ValueStateDescriptor<Tuple2<Long, Long>> descriptor =
                new ValueStateDescriptor<>(
                        "average", //状态名称
                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}), //类型信息
                        Tuple2.of(0L, 0L)); //状态的默认值
        sum = getRuntimeContext().getState(descriptor);//获取状态
    }
}

env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L))
        .keyBy(0)
        .flatMap(new CountWindowAverage())
        .print();

//结果会打印出 (1,4) 和 (1,5)

复制
```

这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。

### State TTL(存活时间)

#### State TTL 介绍

TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：

```java
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.time.Time;

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.seconds(1))
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();

ValueStateDescriptor<String> stateDescriptor = new ValueStateDescriptor<>("zhisheng", String.class);
stateDescriptor.enableTimeToLive(ttlConfig);    //开启 ttl

复制
```

上面配置中有几个选项需要注意：

1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。

2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：

- StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新
- StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新

3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：

- StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值
- StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回

在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.

另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。

注意：

- 状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。
- 目前仅支持参考 processing time 的 TTL
- 使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。
- TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。
- 只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。

#### 清除过期 state

默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。

注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。

此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：

```java
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.time.Time;

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.seconds(1))
    .cleanupFullSnapshot()
    .build();

复制
```

此配置不适用于 RocksDB 状态后端中的增量 checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。

除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：

```java
import org.apache.flink.api.common.state.StateTtlConfig;
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.seconds(1))
    .cleanupInBackground()
    .build();

复制
```

要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。

我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。

在该类中的属性有如下：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143816.png)

- DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig
- UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选
- StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired
- TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选
- Time：设置 TTL 的时间，这里有两个参数 unit 和 size
- CleanupStrategies：TTL 清理策略，在该类中又有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL*STATE*SCAN*SNAPSHOT、INCREMENTAL*CLEANUP 和 ROCKSDB*COMPACTION*FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器）。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144111.png)

如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144324.png)

### 如何使用托管 Operator State

为了使用托管的 Operator State，必须要有一个有状态的函数，这个函数可以实现 CheckpointedFunction 或者 ListCheckpointed 接口。

下面分别讲一下如何使用：

#### CheckpointedFunction

如果是实现 CheckpointedFunction 接口的话，那么我们先来看下这个接口里面有什么方法呢：

```java
//当请求 checkpoint 快照时，将调用此方法
void snapshotState(FunctionSnapshotContext context) throws Exception;

//在分布式执行期间创建并行功能实例时，将调用此方法。 函数通常在此方法中设置其状态存储数据结构
void initializeState(FunctionInitializationContext context) throws Exception;

复制
```

当有请求执行 checkpoint 的时候，snapshotState() 方法就会被调用，initializeState() 方法会在每次初始化用户定义的函数时或者从更早的 checkpoint 恢复的时候被调用，因此 initializeState() 不仅是不同类型的状态被初始化的地方，而且还是 state 恢复逻辑的地方。

目前，List 类型的托管状态是支持的，状态被期望是一个可序列化的对象的 List，彼此独立，这样便于重分配，换句话说，这些对象是可以重新分配的 non-keyed state 的最小粒度，根据状态的访问方法，定义了重新分配的方案：

- Even-split redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或者恢复的时候，这个状态元素列表会被按照并行度分为子列表，每个算子会得到一个子列表。这个子列表可能为空，或包含一个或多个元素。举个例子，如果使用并行性 1，算子的检查点状态包含元素 element1 和 element2，当将并行性增加到 2 时，element1 可能最终在算子实例 0 中，而 element2 将转到算子实例 1 中。
- Union redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或恢复的时候，每个算子都会获得完整的状态元素列表。

如下示例是一个有状态的 SinkFunction 使用 CheckpointedFunction 来发送到外部之前缓存数据，使用了Even-split策略。

下面是一个有状态的 SinkFunction 的示例，它使用 CheckpointedFunction 来缓存数据，然后再将这些数据发送到外部系统，使用了 Even-split 策略：

```java
public class BufferingSink implements SinkFunction<Tuple2<String, Integer>>, CheckpointedFunction {

    private final int threshold;

    private transient ListState<Tuple2<String, Integer>> checkpointedState;

    private List<Tuple2<String, Integer>> bufferedElements;

    public BufferingSink(int threshold) {
        this.threshold = threshold;
        this.bufferedElements = new ArrayList<>();
    }

    @Override
    public void invoke(Tuple2<String, Integer> value, Context contex) throws Exception {
        bufferedElements.add(value);
        if (bufferedElements.size() == threshold) {
            for (Tuple2<String, Integer> element: bufferedElements) {
                //将数据发到外部系统
            }
            bufferedElements.clear();
        }
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {
        checkpointedState.clear();
        for (Tuple2<String, Integer> element : bufferedElements) {
            checkpointedState.add(element);
        }
    }

    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {
        ListStateDescriptor<Tuple2<String, Integer>> descriptor =
            new ListStateDescriptor<>(
                "buffered-elements",
                TypeInformation.of(new TypeHint<Tuple2<String, Integer>>() {}));

        checkpointedState = context.getOperatorStateStore().getListState(descriptor);

        if (context.isRestored()) {
            for (Tuple2<String, Integer> element : checkpointedState.get()) {
                bufferedElements.add(element);
            }
        }
    }
}

复制
```

initializeState 方法将 FunctionInitializationContext 作为参数，它用来初始化 non-keyed 状态。注意状态是如何初始化的，类似于 Keyed state，StateDescriptor 包含状态名称和有关状态值的类型的信息：

```java
ListStateDescriptor<Tuple2<String, Integer>> descriptor =
    new ListStateDescriptor<>(
        "buffered-elements",
        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}));

checkpointedState = context.getOperatorStateStore().getListState(descriptor);

复制
```

#### ListCheckpointed

是一种受限的 CheckpointedFunction，只支持 List 风格的状态和 even-spit 的重分配策略。该接口里面的方法有：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144503.png)

- snapshotState(): 获取函数的当前状态。状态必须返回此函数先前所有的调用结果。
- restoreState(): 将函数或算子的状态恢复到先前 checkpoint 的状态。此方法在故障恢复后执行函数时调用。如果函数的特定并行实例无法恢复到任何状态，则状态列表可能为空。

### Stateful Source Functions

与其他算子相比，有状态的 source 函数需要注意的地方更多，比如为了保证状态的更新和结果的输出原子性，用户必须在 source 的 context 上加锁。

```java
public static class CounterSource extends RichParallelSourceFunction<Long> implements ListCheckpointed<Long> {

    //一次语义的当前偏移量
    private Long offset = 0L;

    //作业取消标志
    private volatile boolean isRunning = true;

    @Override
    public void run(SourceContext<Long> ctx) {
        final Object lock = ctx.getCheckpointLock();

        while (isRunning) {
            //输出和状态更新是原子性的
            synchronized (lock) {
                ctx.collect(offset);
                offset += 1;
            }
        }
    }

    @Override
    public void cancel() {
        isRunning = false;
    }

    @Override
    public List<Long> snapshotState(long checkpointId, long checkpointTimestamp) {
        return Collections.singletonList(offset);
    }

    @Override
    public void restoreState(List<Long> state) {
        for (Long s : state)
            offset = s;
    }
}

复制
```

或许有些算子想知道什么时候 checkpoint 全部做完了，可以参考使用 org.apache.flink.runtime.state.CheckpointListener 接口来实现，在该接口里面有 notifyCheckpointComplete 方法。

### Broadcast State

#### Broadcast State 如何使用

前面提到了两种 Operator state 支持的动态扩展方法：even-split redistribution 和 union redistribution。Broadcast State 是 Flink 支持的另一种扩展方式，它用来支持将某一个流的数据广播到下游所有的 Task 中，数据都会存储在下游 Task 内存中，接收到广播的数据流后就可以在操作中利用这些数据，一般我们会将一些规则数据进行这样广播下去，然后其他的 Task 也都能根据这些规则数据做配置，更常见的就是规则动态的更新，然后下游还能够动态的感知。

Broadcast state 的特点是：

- 使用 Map 类型的数据结构
- 仅适用于同时具有广播流和非广播流作为数据输入的特定算子
- 可以具有多个不同名称的 Broadcast state

那么我们该如何使用 Broadcast State 呢？下面通过一个例子来讲解一下，在这个例子中，我要广播的数据是监控告警的通知策略规则，然后下游拿到我这个告警通知策略去判断哪种类型的告警发到哪里去，该使用哪种方式来发，静默时间多长等。

第一个数据流是要处理的数据源，流中的对象具有告警或者恢复的事件，其中用一个 type 字段来标识哪个事件是告警，哪个事件是恢复，然后还有其他的字段标明是哪个集群的或者哪个项目的，简单代码如下：

```java
DataStreamSource<AlertEvent> alertData = env.addSource(new FlinkKafkaConsumer011<>("alert",
        new AlertEventSchema(),
        parameterTool.getProperties()));

复制
```

然后第二个数据流是要广播的数据流，它是告警通知策略数据（定时从 MySQL 中读取的规则表），简单代码如下：

```java
DataStreamSource<Rule> alarmdata = env.addSource(new GetAlarmNotifyData());

// MapState 中保存 (RuleName, Rule) ，在描述类中指定 State name
MapStateDescriptor<String, Rule> ruleStateDescriptor = new MapStateDescriptor<>(
            "RulesBroadcastState",
            BasicTypeInfo.STRING_TYPE_INFO,
            TypeInformation.of(new TypeHint<Rule>() {}));

// alarmdata 使用 MapStateDescriptor 作为参数广播，得到广播流
BroadcastStream<Rule> ruleBroadcastStream = alarmdata.broadcast(ruleStateDescriptor);

复制
```

然后你要做的是将两个数据流进行连接，连接后再根据告警规则数据流的规则数据进行处理（这个告警的逻辑很复杂，我们这里就不再深入讲），伪代码大概如下：

```java
alertData.connect(ruleBroadcastStream)
    .process(
        new KeyedBroadcastProcessFunction<AlertEvent, Rule>() {
            //根据告警规则的数据进行处理告警事件
        }
    )
    //可能还有更多的操作

复制
```

`alertData.connect(ruleBroadcastStream)` 该 connect 方法将两个流连接起来后返回一个 BroadcastConnectedStream 对象，如果对 BroadcastConnectedStream 不太清楚的可以回看下文章 [4如何使用 DataStream API 来处理数据？]() 再次复习一下。BroadcastConnectedStream 调用 process() 方法执行处理逻辑，需要指定一个逻辑实现类作为参数，具体是哪种实现类取决于非广播流的类型：

- 如果非广播流是 keyed stream，需要实现 KeyedBroadcastProcessFunction
- 如果非广播流是 non-keyed stream，需要实现 BroadcastProcessFunction

那么该怎么获取这个 Broadcast state 呢，它需要通过上下文来获取:

```java
ctx.getBroadcastState(ruleStateDescriptor)

复制
```

#### BroadcastProcessFunction 和 KeyedBroadcastProcessFunction

这两个抽象函数有两个相同的需要实现的接口:

- processBroadcastElement()：处理广播流中接收的数据元
- processElement()：处理非广播流数据的方法

用于处理非广播流是 non-keyed stream 的情况:

```java
public abstract class BroadcastProcessFunction<IN1, IN2, OUT> extends BaseBroadcastProcessFunction {

    public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector<OUT> out) throws Exception;

    public abstract void processBroadcastElement(IN2 value, Context ctx, Collector<OUT> out) throws Exception;
}

复制
```

用于处理非广播流是 keyed stream 的情况

```java
public abstract class KeyedBroadcastProcessFunction<KS, IN1, IN2, OUT> {

    public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector<OUT> out) throws Exception;

    public abstract void processBroadcastElement(IN2 value, Context ctx, Collector<OUT> out) throws Exception;

    public void onTimer(long timestamp, OnTimerContext ctx, Collector<OUT> out) throws Exception;
}

复制
```

可以看到这两个接口提供的上下文对象有所不同。非广播方（processElement）使用 ReadOnlyContext，而广播方（processBroadcastElement）使用 Context。这两个上下文对象（简称 ctx）通用的方法接口有：

- 访问 Broadcast state：ctx.getBroadcastState(MapStateDescriptor stateDescriptor)
- 查询数据元的时间戳：ctx.timestamp()
- 获取当前水印：ctx.currentWatermark()
- 获取当前处理时间：ctx.currentProcessingTime()
- 向旁侧输出（side-outputs）发送数据：ctx.output(OutputTag outputTag, X value)

这两者不同之处在于对 Broadcast state 的访问限制：广播方对其具有读和写的权限（read-write），非广播方只有读的权限（read-only），为什么要这么设计呢，主要是为了保证 Broadcast state 在算子的所有并行实例中是相同的。由于 Flink 中没有跨任务的通信机制，在一个任务实例中的修改不能在并行任务间传递，而广播端在所有并行任务中都能看到相同的数据元，只对广播端提供可写的权限。同时要求在广播端的每个并行任务中，对接收数据的处理是相同的。如果忽略此规则会破坏 State 的一致性保证，从而导致不一致且难以诊断的结果。也就是说，processBroadcast() 的实现逻辑必须在所有并行实例中具有相同的确定性行为。

#### 使用 Broadcast state 需要注意

前面介绍了 Broadcast state，并将 BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 做了个对比，那么接下来强调一下使用 Broadcast state 时需要注意的事项：

- 没有跨任务的通信，这就是为什么只有广播方可以修改 Broadcast state 的原因。
- 用户必须确保所有任务以相同的方式为每个传入的数据元更新 Broadcast state，否则可能导致结果不一致。
- 跨任务的 Broadcast state 中的事件顺序可能不同，虽然广播的元素可以保证所有元素都将转到所有下游任务，但元素到达的顺序可能不一致。因此，Broadcast state 更新不能依赖于传入事件的顺序。
- 所有任务都会把 Broadcast state 存入 checkpoint，虽然 checkpoint 发生时所有任务都具有相同的 Broadcast state。这是为了避免在恢复期间所有任务从同一文件中进行恢复（避免热点），然而代价是 state 在 checkpoint 时的大小成倍数（并行度数量）增加。
- Flink 确保在恢复或改变并行度时不会有重复数据，也不会丢失数据。在具有相同或改小并行度后恢复的情况下，每个任务读取其状态 checkpoint。在并行度增大时，原先的每个任务都会读取自己的状态，新增的任务以循环方式读取前面任务的检查点。
- 不支持 RocksDB state backend，Broadcast state 在运行时保存在内存中。

### Queryable State

Queryable State，顾名思义，就是可查询的状态。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-075631.jpg)

传统管理这些状态的方式是通过将计算后的状态结果存储在第三方 KV 存储中，然后由第三方应用去获取这些 KV 状态，但是在 Flink 种，现在有了 Queryable State，意味着允许用户对流的内部状态进行实时查询。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-091521.jpg)

那么就不再像其他流计算框架，需要将结果存储到其他外部存储系统才能够被查询到，这样我们就可以不再需要等待状态写入外部存储（这块可能是其他系统的主要瓶颈之一），甚至可以做到无需任何数据库就可以让用户直接查询到数据，这使得数据获取到的时间会更短，更及时，如果你有这块的需求（需要将某些状态数据进行展示，比如数字大屏），那么就强烈推荐使用 Queryable State。目前可查询的 state 主要针对可分区的 state，如 keyed state 等。

在 Flink 源码中，为此还专门有一个 module 来讲 Queryable State 呢！

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144649.png)

那么我们该如何使用 Queryable State 呢？有如下两种方式 ：

- QueryableStateStream, 将 KeyedStream 转换为 QueryableStateStream，类似于 Sink，后续不能进行任何转换操作
- StateDescriptor#setQueryable(String queryableStateName)，将 Keyed State 设置为可查询的 （不支持 Operator State）

外部应用在查询 Flink 应用程序内部状态的时候要使用 QueryableStateClient, 提交异步查询请求来获取状态。如何使状态可查询呢，假如已经创建了一个状态可查询的 Job，并通过 JobClient 提交 Job，那么它在 Flink 内部的具体实现如下图（图片来自 [Queryable States in ApacheFlink - How it works](http://vishnuviswanath.com/flink_queryable_state1.html)）所示：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-073842.jpg)

上面讲解了让 State 可查询的原理，如果要在 Flink 集群中使用的话，首先得将 Flink 安装目录下 opt 里面的 `flink-queryable-state-runtime_2.11-1.9.0.jar` 复制到 lib 目录下，默认 lib 目录是不包含这个 jar 的。

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144825.png)

然后你可以像下面这样操作让状态可查询：

```java
// Reducing state
ReducingStateDescriptor<Tuple2<Integer, Long>> reducingState = new ReducingStateDescriptor<>(
        "zhisheng",
        new SumReduce(),
        source.getType());

final String queryName = "zhisheng";

final QueryableStateStream<Integer, Tuple2<Integer, Long>> queryableState =
        dataStream.keyBy(new KeySelector<Tuple2<Integer, Long>, Integer>() {
            private static final long serialVersionUID = -4126824763829132959L;
            @Override
            public Integer getKey(Tuple2<Integer, Long> value) {
                return value.f0;
            }
        }).asQueryableState(queryName, reducingState);

复制
```

除了上面的 Reducing，你还可以使用 ValueState、FoldingState，还可以直接通过asQueryableState(queryName），注意不支持 ListState，调用 asQueryableState 方法后会返回 QueryableStateStream，接着无需再做其他操作。

那么用户如果定义了 Queryable State 的话，该怎么来查询对应的状态呢？下面来看看具体逻辑：

![img](http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-074814.jpg)

简单来说，当用户在 Job 中定义了 queryable state 之后，就可以在外部通过QueryableStateClient 来查询对应的状态实时值，你可以创建如下方法：

```java
//创建 Queryable State Client
QueryableStateClient client = new QueryableStateClient(host, port);

public QueryableStateClient(final InetAddress remoteAddress, final int remotePort) {
    ...
    this.client = new Client<>(
            "Queryable State Client", 1,
            messageSerializer, new DisabledKvStateRequestStats());
}

复制
```

在 QueryableStateClient 中有几个不同参数的 getKvState 方法，参数可有 JobID、queryableStateName、key、namespace、keyTypeInfo、namespaceTypeInfo、StateDescriptor，其实内部最后调用的是一个私有的 getKvState 方法：

```java
private CompletableFuture<KvStateResponse> getKvState(
        final JobID jobId, final String queryableStateName,
        final int keyHashCode, final byte[] serializedKeyAndNamespace) {
    ...
    //构造 KV state 查询的请求
    KvStateRequest request = new KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);
    //这个 client 是在构造 QueryableStateClient 中赋值的，这个 client 是 Client<KvStateRequest, KvStateResponse>，发送请求后会返回 CompletableFuture<KvStateResponse>
    return client.sendRequest(remoteAddress, request);
    ...
}

复制
```

在 Flink 源码中专门有一个 QueryableStateOptions 类来设置可查询状态相关的配置，有如下这些配置。

服务器端：

- queryable-state.proxy.ports：可查询状态代理的服务器端口范围的配置参数，默认是 9069
- queryable-state.proxy.network-threads：客户端代理的网络线程数，默认是 0
- queryable-state.proxy.query-threads：客户端代理的异步查询线程数，默认是 0
- queryable-state.server.ports：可查询状态服务器的端口范围，默认是 9067
- queryable-state.server.network-threads：KvState 服务器的网络线程数
- queryable-state.server.query-threads：KvStateServerHandler 的异步查询线程数
- queryable-state.enable：是否启用可查询状态代理和服务器

客户端：

- queryable-state.client.network-threads：KvState 客户端的网络线程数

**注意**：

可查询状态的生命周期受限于 Job 的生命周期，例如，任务在启动时注册可查询状态，在清理的时候会注销它。在未来的版本中，可能会将其解耦，以便在任务完成后仍可以允许查询到任务的状态。

## FlinkSQL的几种Join类型。

Window Join和Interval Join。Window Join又可以根据Window的类型细分出：Tumbling Window Join、Sliding Window Join、Session Widnow Join。
Windows类型的join都是利用window的机制，先将数据缓存在Window State中，当窗口触发计算时，执行join操作；
interval join也是利用state存储数据再处理，区别在于state中的数据有失效机制，依靠数据触发数据清理；
目前Stream join的结果是数据的笛卡尔积；



[Flink 双流 Join 的3种操作示例](https://mp.weixin.qq.com/s/vTAkUrPfO4DW1qOzJ-Zs4A)

## flink checkpoint

## flink 精准一次性语义实现

[硬核！八张图搞懂 Flink 端到端精准一次处理语义 Exactly-once](https://cloud.tencent.com/developer/article/1783222)

## flink多流join

flink cdc实现

flink调优

[Flink剖析系列之Yarn Session Cluster 和 Yarn Per Job 模式作业提交流程分析](https://blog.lovedata.net/761a39dc.html)

## Flink 原理与实现：Aysnc I/O

[Flink 原理与实现：Aysnc I/O](http://wuchong.me/blog/2017/05/17/flink-internals-async-io/)

## Flink 原理与实现：内存管理

[Flink 原理与实现：内存管理](http://wuchong.me/blog/2016/04/29/flink-internals-memory-manage/)



# Flinkx

## Flinkx工作原理

## Flinkx任务启动流程及部分源码



# 其他

1、大数据平台售前解决方案、标书编写。

## Kerberos协议基本原理。

## Git基本使用。

## 数据中台建设方法论

## Linux常用的操作命令

(日志截取、管道命令、磁盘存储、top、vim)

## MySQL binlog

https://zhuanlan.zhihu.com/p/33504555

[bin-log 日志清理方式](https://www.cnblogs.com/zhanmeiliang/p/5951478.html)

https://blog.csdn.net/csdnhsh/article/details/90762682

## JVM

[堆内内存与堆外内存](https://www.cnblogs.com/ronnieyuan/p/11718536.html)

[常用的 JVM 性能调优监控工具](https://mp.weixin.qq.com/s/-Ipywa11EbkdHi3lZV5kRA)

## RPC通信

[RPC基本原理以及如何用Netty来实现RPC](https://www.jianshu.com/p/8876c9f3cd7f)



# 问题记录（* 标注重点）

## 数仓

### * 数仓0-1建设

通过三步调研（业务调研、需求调研、数据调研），划分主题域

业务调研初步确定dw层主题域，

需求调研初步确定应用层主题域，

数据调研，整理现有数据源、数据字典，形成数据调研表，确定每张表所属的业务线，确定ods层数据表的主题域

然后构建总线矩阵，明确业务过程所属主题域，业务过程和维度的关系。维度建模（星型模型、四步建模）。

设计数仓分层架构（ods-dwd-dws-ads），

定制规范（命名规范、模型规范、开发规范、流程规范）。

数据治理（数据质量，数据安全，元数据管理）。

开工ETL/BI，迭代开发。

### 缓慢变化维处理方式

[缓慢变化维的10种处理方式](https://blog.csdn.net/qq_36039236/article/details/107819326)

### 一致性维度

### * 如何保证指标一致性

### * 如何保证数据质量

数据质量，ods到dwd，etl做了哪些工作，通过sparkSQL处理，
上游的数据到采集端，数据质量规范化，怎么预防和解决，对哪些表、字段、做什么方式的校验
告警的流程。怎么处理

### * 元数据管理怎么做的

元数据治理的项目和理解(元数据采集，备份，依赖、表级、database级的治理)，比如数仓下游的出现问题，如何定位、解决问题，怎么样去做机制、流程、产品化

### 数据血缘怎么做的

### 指标管理体系怎么做的

### 开发规范有哪些

### 数据如何为业务赋能

### 实时数仓建设技术选型

[Flink + Iceberg 全场景实时数仓的建设实践](https://mp.weixin.qq.com/s/xBHhqDQHDaLZYX_jpvZDcQ)

### Kappa & lambda架构

**lambda架构痛点**

- 同时维护实时平台和离线平台两套引擎，运维成本高
- 实时离线两个平台需要维护两套框架不同但业务逻辑相同代码，开发成本高
- 数据有两条不同链路，容易造成数据的不一致性
- 数据更新成本大，需要重跑链路

![image-20210316114940424](https://tva1.sinaimg.cn/large/e6c9d24ely1gollqoxt41j20ip09jwhk.jpg)

**Kappa 架构的痛点**

- 首先，在构建实时业务场景时，会用到 Kappa 去构建一个近实时的场景，但如果想对数仓中间层例如 ODS 层做一些简单的 OLAP 分析或者进一步的数据处理时，如将数据写到 DWD 层的 Kafka，则需要另外接入 Flink。同时，当需要从 DWD 层的 Kafka 把数据再导入到 Clickhouse，Elasticsearch，MySQL 或者是 Hive 里面做进一步的分析时，显然就增加了整个架构的复杂性。
- 其次，Kappa 架构是强烈依赖消息队列的，我们知道消息队列本身在整个链路上数据计算的准确性是严格依赖它上游数据的顺序，消息队列接的越多，发生乱序的可能性就越大。ODS 层数据一般是绝对准确的，把 ODS 层的数据发送到下一个 kafka  的时候就有可能发生乱序，DWD 层再发到 DWS 的时候可能又乱序了，这样数据不一致性就会变得很严重。
- 第三，Kafka 由于它是一个顺序存储的系统，顺序存储系统是没有办法直接在其上面利用 OLAP 分析的一些优化策略，例如谓词下推这类的优化策略，在顺序存储的 Kafka 上来实现是比较困难的事情。

![image-20210316115059673](https://tva1.sinaimg.cn/large/e6c9d24ely1golls2osjlj20ip09nq5k.jpg)

- 对消息队列存储要求高，消息队列的回溯能力不及离线存储
- 消息队列本身对数据存储有时效性，且当前无法使用 OLAP 引擎直接分析消息队列中的数据
- 全链路依赖消息队列的实时计算可能因为数据的时序性导致结果不正确

## flink

### * flink checkpoint的理解

### * flink 怎么实现exactly once 语义

(flink内部节点数据处理不一致，上下游节点，上游有n，下游n+1比n先到，比如kafka的offset的对齐，barrier，借助前后两次ck记录offset，下游做对齐offset，再去消费数据)

https://cloud.tencent.com/developer/article/1783222

### flink  watermark

### * 实时数据反压如何处理

1.5版本之后有处理机制

### flink调优

### flink多流join

### flink cdc实现

### clickhouse原理

### kylin原理及调优



## 项目

### 中航金网项目

平台的迁移，做了多久，遇到的比较大的问题
sql兼容性问题，大量任务修改迁移，怎么处理
flink做了哪些东西
flink的复杂事件的了解
kafka 和 mq的区别



### 城市大脑

















